{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Averaging ATLAS Light Curves\n",
    "### Includes chi-square cut, uncertainty cut, control light curve cut, and averaging\n",
    "\n",
    "This iPython notebook will help you apply each cut with a greater degree of control than an automatic cleaning. You will be walked through the following:\n",
    "1. Correction for ATLAS reference template changes\n",
    "2. Static uncertainty cut\n",
    "3. Estimating true uncertainties\n",
    "4. Dynamic chi-square cut\n",
    "5. Control light curve cut\n",
    "6. Averaging the light curve and cutting bad bins\n",
    "\n",
    "After running a cell, the descriptions located above that cell will help you interpret the plots and make decisions about the supernova.\n",
    "\n",
    "In order for this notebook to work correctly, the ATLAS light curves must already be downloaded and saved. Each light curve must also only include measurements for a single filter.\n",
    "\n",
    "## Step 1: Load the ATLAS light curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOADING THE SN LIGHT CURVE #####\n",
    "\n",
    "# Enter the target SN name:\n",
    "tnsname = ''\n",
    "\n",
    "# Enter the SN light curve file name:\n",
    "filename = ''\n",
    "\n",
    "# Enter the filter for this light curve (must be 'o' or 'c'):\n",
    "filter = ''\n",
    "\n",
    "# Optionally, enter the SN's discovery date (if None is entered, it will be \n",
    "# fetched automatically from TNS using the API key, TNS ID, and bot name):\n",
    "discdate = None\n",
    "api_key = None\n",
    "tns_id = None\n",
    "bot_name = None\n",
    "\n",
    "# Enter the number of minimum days between a template change date and the SN discovery date \n",
    "# in order to use this data as baseline (meaning before SN starts) flux for that template region:\n",
    "Ndays_min = 6\n",
    "\n",
    "##### LOADING CONTROL LIGHT CURVES #####\n",
    "\n",
    "# Set to True if you are planning on applying the control light curve cut \n",
    "# and have already downloaded the control light curves:\n",
    "load_controls = True\n",
    "\n",
    "# Enter the number of control light curves to load:\n",
    "Ncontrols = 8\n",
    "\n",
    "# Enter the source directory of the control light curve files:\n",
    "controls_dir = 'controls'\n",
    "\n",
    "##### DEFAULT SETTINGS FOR PLOTTING #####\n",
    "\n",
    "# If True, try to calculate the best y limits automatically for each plot;\n",
    "# if False, leave y limits to matplotlib \n",
    "auto_xylimits = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules, set preliminary variables, etc.\n",
    "\n",
    "import sys, re\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# storing, accessing, and manipulating the light curve\n",
    "import pandas as pd\n",
    "from pdastro import pdastrostatsclass, AandB, AnotB, AorB, not_AandB\n",
    "\n",
    "# getting discovery date from TNS\n",
    "import requests, json\n",
    "from collections import OrderedDict\n",
    "from astropy.time import Time\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as matlib\n",
    "import warnings\n",
    "warnings.simplefilter('error', RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# plotting styles\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=13)\n",
    "plt.rc('ytick', labelsize=13)\n",
    "plt.rc('legend', fontsize=13)\n",
    "plt.rc('font', size=13)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# ATLAS template changes\n",
    "global tchange1\n",
    "global tchange2\n",
    "tchange1 = 58417\n",
    "tchange2 = 58882\n",
    "\n",
    "# dictionary for storing light curve and other important information\n",
    "global lc_info\n",
    "lc_info = {}\n",
    "\n",
    "# dictionary for optionally storing control light curves\n",
    "global controls\n",
    "controls = {}\n",
    "\n",
    "# list for storing the new dflux column name ('duJy' vs. 'duJy_new')\n",
    "global dflux_colnames\n",
    "dflux_colnames = ['duJy'] * (Ncontrols+1)\n",
    "\n",
    "# flag values for updating 'Mask' column with\n",
    "flag_chisquare = 0x1\n",
    "flag_uncertainty = 0x2\n",
    "flag_controls_bad = 0x400000\n",
    "flag_controls_questionable = 0x80000\n",
    "flag_controls_x2 = 0x100\n",
    "flag_controls_stn = 0x200\n",
    "flag_controls_Nclip = 0x400\n",
    "flag_controls_Ngood = 0x800\n",
    "flag_badday = 0x800000\n",
    "flag_ixclip = 0x1000\n",
    "flag_smallnum = 0x2000\n",
    "\n",
    "# get discovery date if needed, load in light curve, account for template changes\n",
    "\n",
    "def get_tns_data(tnsname, api_key, tns_id, bot_name):\n",
    "\ttry:\n",
    "\t\tget_obj = [(\"objname\",tnsname), (\"objid\",\"\"), (\"photometry\",\"1\"), (\"spectra\",\"1\")]\n",
    "\t\tget_url = 'https://www.wis-tns.org/api/get/object'\n",
    "\t\tjson_file = OrderedDict(get_obj)\n",
    "\t\tget_data = {'api_key':api_key,'data':json.dumps(json_file)}\n",
    "\t\tresponse = requests.post(get_url, data=get_data, headers={'User-Agent':'tns_marker{\"tns_id\":%s,\"type\": \"bot\", \"name\":\"%s\"}' % (str(tns_id), str(bot_name))})\n",
    "\t\tjson_data = json.loads(response.text,object_pairs_hook=OrderedDict)\n",
    "\t\treturn json_data\n",
    "\texcept Exception as e:\n",
    "\t\treturn 'Error: \\n'+str(e)\n",
    "\n",
    "def get_discdate(tnsname, api_key):\n",
    "\tjson_data = get_tns_data(tnsname, api_key, tns_id, bot_name)\n",
    "\tdiscoverydate = json_data['data']['reply']['discoverydate']\n",
    "\tdate = list(discoverydate.partition(' '))[0]\n",
    "\ttime = list(discoverydate.partition(' '))[2]\n",
    "\tdisc_date_format = date+'T'+time\n",
    "\tdateobjects = Time(disc_date_format, format='isot', scale='utc')\n",
    "\tdisc_date = dateobjects.mjd\n",
    "\treturn disc_date\n",
    "\n",
    "def get_xth_percentile_flux(lc_type, percentile, indices=None):\n",
    "    if indices is None:\n",
    "        indices = lc_info[lc_type].getindices()\n",
    "    if len(indices)==0: \n",
    "        return None\n",
    "    else:\n",
    "        return np.percentile(lc_info[lc_type].t.loc[indices, 'uJy'], percentile)\n",
    "\t\n",
    "def drop_extra_columns(lc_type):\n",
    "\tdropcols=[]\n",
    "\tif 'Noffsetlc' in lc_info[lc_type].t.columns: dropcols.append('Noffsetlc')\n",
    "\tif '__tmp_SN' in lc_info[lc_type].t.columns: dropcols.append('__tmp_SN')\n",
    "\tfor col in lc_info[lc_type].t.columns:\n",
    "\t\tif re.search('^c\\d_',col): \n",
    "\t\t\tdropcols.append(col)\n",
    "\tif len(dropcols)>0: \n",
    "\t\tprint('Dropping extra columns: ',dropcols)\n",
    "\t\tlc_info[lc_type].t.drop(columns=dropcols,inplace=True)\n",
    "\n",
    "def load_lc(filename):\n",
    "\tlc_info['lc'] = pdastrostatsclass()\n",
    "\ttry:\n",
    "\t\tprint('\\nLoading SN %s light curve at %s and clearing previous flags in \"Mask\" column...' % (lc_info['tnsname'], filename))\n",
    "\t\tlc_info['lc'].load_spacesep(filename,delim_whitespace=True)\n",
    "\texcept Exception as e:\n",
    "\t\traise RuntimeError('Could not load light curve for SN %s at %s: %s' % (lc_info['tnsname'], filename, str(e)))\n",
    "\t\n",
    "\tlc_info['baseline_ix'] = lc_info['lc'].ix_inrange(colnames=['MJD'],uplim=lc_info['discdate']-20,exclude_uplim=True)\n",
    "\tif len(lc_info['baseline_ix'])<=0:\n",
    "\t\traise RuntimeError('Baseline length is 0! Exiting...')\n",
    "\tlc_info['duringsn_ix'] = AnotB(lc_info['lc'].getindices(),lc_info['baseline_ix'])\n",
    "\n",
    "def load_control_lcs(controls_dir, Ncontrols): \n",
    "\tprint('\\nLoading control light curves and clearing previous flags in \"Mask\" column...')\n",
    "\tfor control_index in range(1,Ncontrols+1):\n",
    "\t\tcontrols[control_index] = pdastrostatsclass()\n",
    "\t\tfilename = controls_dir + '/' + lc_info['tnsname'] + '_i%03d.'%control_index + lc_info['filter'] + '.lc.txt'\n",
    "\t\tprint('# Loading control light curve at ',filename)\n",
    "\t\tcontrols[control_index].load_spacesep(filename,delim_whitespace=True)\n",
    "\n",
    "\t\t# clear any previous control light curve flags\n",
    "\t\tcontrols[control_index].t['Mask'] = 0 #np.bitwise_and(controls[control_index].t['Mask'],(flag_chisquare|flag_uncertainty))\n",
    "\n",
    "def save_lc(lc_type, filename, overwrite=False):\n",
    "    print('Saving light curve at %s' % filename)\n",
    "    lc_info[lc_type].write(filename,overwrite=overwrite)\n",
    "\n",
    "def verify_mjds(Ncontrols):\n",
    "    print()\n",
    "    # sort sn lc by mjd\n",
    "    mjd_sorted_i = lc_info['lc'].ix_sort_by_cols('MJD')\n",
    "    lc_info['lc'].t = lc_info['lc'].t.loc[mjd_sorted_i]\n",
    "    sn_sorted = lc_info['lc'].t.loc[mjd_sorted_i,'MJD'].to_numpy()\n",
    "\n",
    "    for control_index in range(1,Ncontrols+1):\n",
    "        # sort control lc by mjd\n",
    "        mjd_sorted_i = controls[control_index].ix_sort_by_cols('MJD')\n",
    "        control_sorted = controls[control_index].t.loc[mjd_sorted_i,'MJD'].to_numpy()\n",
    "        \n",
    "        # compare control lc to sn lc and, if out of agreement, fix\n",
    "        if (len(sn_sorted) != len(control_sorted)) or (np.array_equal(sn_sorted, control_sorted) is False):\n",
    "            print('MJDs out of agreement for control light curve %03d, fixing...' % control_index)\n",
    "\n",
    "            mjds_onlysn = AnotB(sn_sorted, control_sorted)\n",
    "            mjds_onlycontrol = AnotB(control_sorted, sn_sorted)\n",
    "\n",
    "            # for the mjds only in sn, add row with that mjd to control lc, with all values of other columns NaN\n",
    "            if len(mjds_onlysn) > 0:\n",
    "                #print('# Adding %d NaN rows to control light curve...' % len(mjds_onlysn))\n",
    "                for mjd in mjds_onlysn:\n",
    "                    controls[control_index].newrow({'MJD':mjd,'Mask':0})\n",
    "            \n",
    "            # remove indices of rows in control lc for which there is no mjd in the sn lc\n",
    "            if len(mjds_onlycontrol) > 0:\n",
    "                #print('# Removing %d control light curve rows without matching SN rows...' % len(mjds_onlycontrol))\n",
    "                indices2skip = []\n",
    "                for mjd in mjds_onlycontrol:\n",
    "                    ix = controls[control_index].ix_equal('MJD',mjd)\n",
    "                    if len(ix)!=1:\n",
    "                        raise RuntimeError(f'# Couldn\\'t find MJD={mjd} in column MJD, but should be there!')\n",
    "                    indices2skip.extend(ix)\n",
    "                indices = AnotB(controls[control_index].getindices(),indices2skip)\n",
    "            else:\n",
    "                indices = controls[control_index].getindices()\n",
    "            \n",
    "            ix_sorted = controls[control_index].ix_sort_by_cols('MJD',indices=indices)\n",
    "            controls[control_index].t = controls[control_index].t.loc[ix_sorted]\n",
    "    print('\\nFinished sorting SN and control light curves')\n",
    "\n",
    "lc_info['tnsname'] = tnsname\n",
    "\n",
    "if filter != 'o' and filter != 'c': \n",
    "\tprint('Filter must be \"o\" or \"c\"!')\n",
    "\tsys.exit()\n",
    "lc_info['filter'] = filter\n",
    "\n",
    "if discdate is None:\n",
    "\tprint('Obtaining SN %s discovery date from TNS...' % lc_info['tnsname'])\n",
    "\tdiscdate = get_discdate(lc_info['tnsname'], api_key)\n",
    "\tprint('Discovery date: ',discdate)\n",
    "lc_info['discdate'] = discdate - 20\n",
    "\n",
    "# new text file that will contain record of each cut, etc.\n",
    "f = open(f'{lc_info[\"tnsname\"]}_output.md', 'w')\n",
    "f.write(f'# SN {lc_info[\"tnsname\"]} Light Curve Cleaning and Averaging\\n\\nFilter: {lc_info[\"filter\"]}-band\\nDiscovery date: {lc_info[\"discdate\"]}\\nNumber of control light curves: {Ncontrols}')\n",
    "\n",
    "load_lc(filename)\n",
    "if load_controls: \n",
    "\tload_control_lcs(controls_dir, Ncontrols)\n",
    "\tverify_mjds(Ncontrols)\n",
    "else:\n",
    "\tNcontrols = 0\n",
    "\n",
    "# Calculate uJy/duJy column\n",
    "print('\\nCalculating uJy/duJy column...')\n",
    "lc_info['lc'].t['uJy/duJy'] = lc_info['lc'].t['uJy']/lc_info['lc'].t['duJy']\n",
    "lc_info['lc'].t = lc_info['lc'].t.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot control light curves underneath SN light curve?\n",
    "plot_controls = True\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the following plot of the loaded light curves:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot SN and control light curves\n",
    "\n",
    "def do_manual_xylimits(limits):\n",
    "    # if any limit is not None:\n",
    "    for limit in limits:\n",
    "        if not(limit is None):\n",
    "            return True\n",
    "    # if all limits are None:\n",
    "    return False\n",
    "\n",
    "def set_xylimits(limits, lc_type='lc'):\n",
    "    if auto_xylimits:\n",
    "        if limits[0] is None:\n",
    "            limits[0] = lc_info[lc_type].t['MJD'].min() * 0.999\n",
    "        if limits[1] is None:\n",
    "            limits[1] = lc_info[lc_type].t['MJD'].max() * 1.001\n",
    "\n",
    "        # exclude measurements with duJy > 160\n",
    "        good_ix = lc_info[lc_type].ix_inrange(colnames='duJy', uplim=160)\n",
    "        # get 5% of abs(max flux - min flux)\n",
    "        flux_min = lc_info[lc_type].t.loc[good_ix, 'uJy'].min()\n",
    "        flux_max = lc_info[lc_type].t.loc[good_ix, 'uJy'].max()\n",
    "        diff = abs(flux_max - flux_min)\n",
    "        offset = 0.05 * diff\n",
    "\n",
    "        if limits[2] is None: \n",
    "            limits[2] = flux_min - offset\n",
    "        if limits[3] is None:\n",
    "            limits[3] = flux_max + offset\n",
    "\n",
    "        return limits\n",
    "    else:\n",
    "        if do_manual_xylimits(limits):\n",
    "            return limits\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def plot_all_lcs(add2title=None, plot_controls=False, templates=False, baseline_ix=None, duringsn_ix=None, dflux_colname='duJy', limits=None):\n",
    "    color = 'orange' if lc_info[\"filter\"] == 'o' else 'cyan'\n",
    "\n",
    "    fig = plt.figure(figsize=(12,6), tight_layout=True)\n",
    "    plt.axhline(linewidth=1,color='k')\n",
    "    plt.ylabel('Flux (µJy)')\n",
    "    plt.xlabel('MJD')\n",
    "    title = f'SN {lc_info[\"tnsname\"]} and control light curves {lc_info[\"filter\"]}-band flux'\n",
    "    if not(add2title is None):\n",
    "        title += add2title\n",
    "    plt.title(title)\n",
    "    if templates:\n",
    "        plt.axvline(x=tchange1, color='magenta', label='ATLAS template change', zorder=30)\n",
    "        plt.axvline(x=tchange2, color='magenta', zorder=30)\n",
    "\n",
    "    if baseline_ix is None:\n",
    "        baseline_ix = lc_info['baseline_ix']\n",
    "    if duringsn_ix is None:\n",
    "        duringsn_ix = lc_info['duringsn_ix']\n",
    "\n",
    "    # set x and y limits\n",
    "    limits = set_xylimits(limits)\n",
    "    if not(limits is None):\n",
    "        plt.xlim(limits[0],limits[1])\n",
    "        plt.ylim(limits[2],limits[3])\n",
    "\n",
    "    if load_controls and plot_controls:\n",
    "        for control_index in range(1,Ncontrols+1):\n",
    "            plt.errorbar(controls[control_index].t['MJD'], controls[control_index].t['uJy'], yerr=controls[control_index].t[dflux_colname], fmt='none', ecolor='blue', elinewidth=1, c='blue', alpha=0.3, zorder=0)\n",
    "            if control_index == 1:\n",
    "                plt.scatter(controls[control_index].t['MJD'], controls[control_index].t['uJy'], s=45,color='blue',marker='o', alpha=0.3, zorder=0, label=f'{Ncontrols} control light curves')\n",
    "            else:\n",
    "                plt.scatter(controls[control_index].t['MJD'], controls[control_index].t['uJy'], s=45,color='blue',marker='o', alpha=0.3, zorder=0)\n",
    "\n",
    "    plt.errorbar(lc_info['lc'].t.loc[baseline_ix,'MJD'], lc_info['lc'].t.loc[baseline_ix,'uJy'], yerr=lc_info['lc'].t.loc[baseline_ix,dflux_colname], fmt='none',ecolor=color, elinewidth=1, c=color, zorder=10)\n",
    "    plt.scatter(lc_info['lc'].t.loc[baseline_ix,'MJD'],lc_info['lc'].t.loc[baseline_ix,'uJy'], s=45, color=color, marker='o', zorder=10, label='Baseline flux')\n",
    "\t\n",
    "    plt.errorbar(lc_info['lc'].t.loc[duringsn_ix,'MJD'], lc_info['lc'].t.loc[duringsn_ix,'uJy'], lc_info['lc'].t.loc[duringsn_ix,dflux_colname], fmt='none', ecolor='red', elinewidth=1, c='red', zorder=20)\n",
    "    plt.scatter(lc_info['lc'].t.loc[duringsn_ix,'MJD'], lc_info['lc'].t.loc[duringsn_ix,'uJy'], s=45, color='red', marker='o', zorder=20, label='SN flux')\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)\n",
    "\n",
    "limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "plot_all_lcs(add2title=' (original)', plot_controls=plot_controls, limits=limits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Correct for ATLAS reference template changes\n",
    "\n",
    "This notebook takes into account ATLAS's periodic replacement of the difference image reference templates, which may cause step discontinuities in flux. Two template changes have been recorded at MJDs 58417 and 58882. More information can be found here: https://fallingstar-data.com/forcedphot/faq/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the number of minimum days between a template change date and the SN discovery date \n",
    "# in order to use this data as baseline flux for that template region:\n",
    "Ndays_min = 6\n",
    "\n",
    "# Plot the light curve before and after correcting for ATLAS reference template changes?:\n",
    "plot = True\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get template change regions and plot \"before\" template change correction\n",
    "\n",
    "def get_Ndays(SN_region_index):\n",
    "    return 200 if SN_region_index == 2 else 40\n",
    "\n",
    "def get_baseline_regions(Ndays_min):\n",
    "    print('Getting region indices around SN... ')\n",
    "    regions = {}\n",
    "    regions['t0']   = lc_info['lc'].ix_inrange(colnames=['MJD'],                  uplim=tchange1)\n",
    "    regions['t1']   = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=tchange1, uplim=tchange2)\n",
    "    regions['t2']   = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=tchange2)\n",
    "    regions['b_t0'] = AandB(regions['t0'], lc_info['baseline_ix'])\n",
    "    regions['b_t1'] = AandB(regions['t1'], lc_info['baseline_ix'])\n",
    "    regions['b_t2'] = AandB(regions['t2'], lc_info['baseline_ix'])\n",
    "\n",
    "    # find region SN starts in \n",
    "    SN_region_index = None\n",
    "    if lc_info['discdate']<= tchange1:\n",
    "        SN_region_index = 0\n",
    "    elif lc_info['discdate'] > tchange1 and lc_info['discdate'] <= tchange2:\n",
    "        SN_region_index = 1\n",
    "    elif lc_info['discdate'] > tchange2:\n",
    "        SN_region_index = 2\n",
    "    if SN_region_index is None:\n",
    "        raise RuntimeError('Could not find region with SN discovery date!')\n",
    "    else:\n",
    "        print('SN discovery date located in template region t%d' % SN_region_index)\n",
    "\n",
    "    # for region with tail end of the SN, get last Ndays days and classify as baseline\n",
    "    adjust_region_index = SN_region_index\n",
    "    if adjust_region_index < 2 and len(regions['b_t%d'%adjust_region_index]) >= Ndays_min:\n",
    "        adjust_region_index += 1\n",
    "    if len(regions['b_t%d'%adjust_region_index]) < Ndays_min:\n",
    "        print('Getting baseline flux for template region t%d by obtaining last %d days of region... ' % (adjust_region_index, get_Ndays(adjust_region_index)))\n",
    "        regions['b_t%d'%adjust_region_index] = lc_info['lc'].ix_inrange(colnames=['MJD'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlowlim=lc_info['lc'].t.loc[regions['t%d'%adjust_region_index][-1],'MJD']- get_Ndays(adjust_region_index),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tuplim=lc_info['lc'].t.loc[regions['t%d'%adjust_region_index][-1],'MJD'])\n",
    "    if adjust_region_index < 1: \n",
    "        regions['b_t1'] = regions['t1']\n",
    "    if adjust_region_index < 2: \n",
    "        regions['b_t2'] = regions['t2']\n",
    "\n",
    "    for region_index in range(0,3):\n",
    "        if len(regions['t%d'%region_index]) > 0:\n",
    "            print('TEMPLATE REGION t%d MJD RANGE: %0.2f - %0.2f' % (region_index, lc_info['lc'].t.loc[regions['t%d'%region_index][0],'MJD'], lc_info['lc'].t.loc[regions['t%d'%region_index][-1],'MJD']))\n",
    "        else:\n",
    "            print('TEMPLATE REGION t%d MJD RANGE: not found' % region_index)\n",
    "        if len(regions['b_t%d'%region_index]) > 0:\n",
    "            print('TEMPLATE REGION b_t%d BASELINE MJD RANGE: %0.2f - %0.2f' % (region_index, lc_info['lc'].t.loc[regions['b_t%d'%region_index][0],'MJD'], lc_info['lc'].t.loc[regions['b_t%d'%region_index][-1],'MJD']))\n",
    "        else:\n",
    "            print('TEMPLATE REGION b_t%d BASELINE MJD RANGE: not found' % region_index)\n",
    "    \n",
    "    # check to make sure baseline flux is still consistent by getting median of first and last halves of affected region\n",
    "    first_i = regions['b_t%d'%adjust_region_index][0]\n",
    "    mid_i   = regions['b_t%d'%adjust_region_index][int(len(regions['b_t%d'%adjust_region_index])/2)]\n",
    "    last_i  = regions['b_t%d'%adjust_region_index][-1]\n",
    "    median1 = np.median(lc_info['lc'].t.loc[lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=lc_info['lc'].t.loc[first_i,'MJD'], uplim=lc_info['lc'].t.loc[mid_i,'MJD']), 'uJy'])\n",
    "    median2 = np.median(lc_info['lc'].t.loc[lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=lc_info['lc'].t.loc[mid_i+1,'MJD'], uplim=lc_info['lc'].t.loc[last_i,'MJD']), 'uJy'])\n",
    "    print('Checking that baseline flux is consistent throughout adjusted region...\\n# Median of first half: %0.2f\\n# Median of second half: %0.2f' % (median1,median2))\n",
    "\n",
    "    lc_info['baseline_rev_ix'] = np.concatenate([regions['b_t0'],regions['b_t1'],regions['b_t2']])\n",
    "    lc_info['durings_rev_ix'] = AnotB(lc_info['lc'].getindices(),lc_info['baseline_rev_ix'])\n",
    "    return regions\n",
    "\n",
    "def set_baseline_region(regions, region_index, start_override=None, end_override=None):\n",
    "\tif not(start_override is None) and not(end_override) is None:\n",
    "\t\tregions['b_t%d'%region_index] = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=start_override, uplim=end_override)\n",
    "\telif not(start_override is None):\n",
    "\t\tregions['b_t%d'%region_index] = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=start_override, uplim=lc_info['lc'].t.loc[regions['b_t%d'%region_index][-1],'MJD'])\n",
    "\telif not(end_override is None):\n",
    "\t\tregions['b_t%d'%region_index] = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=lc_info['lc'].t.loc[regions['b_t%d'%region_index][0],'MJD'], uplim=end_override)\n",
    "\treturn regions\n",
    "\n",
    "def set_baseline_regions(regions):\n",
    "\tfor region_index in range(0,3):\n",
    "\t\tstart_override = input('Override template region b_t%d START MJD (n to skip): ' % region_index)\n",
    "\t\tend_override = input('Override template region b_t%d END MJD (n to skip): ' % region_index)\n",
    "\n",
    "\t\tif start_override.isdigit() and end_override.isdigit():\n",
    "\t\t\tprint('# Overriding START for template region b_t%d with %f and END with %f: ' % (region_index,float(start_override),float(end_override)))\n",
    "\t\t\tregions = set_baseline_region(regions, region_index, start_override=float(start_override), end_override=float(end_override))\n",
    "\t\telif start_override.isdigit():\n",
    "\t\t\tprint('# Overriding START for template region b_t%d with %f: ' % (region_index,float(start_override)))\n",
    "\t\t\tregions = set_baseline_region(regions, region_index, start_override=float(start_override))\n",
    "\t\telif end_override.isdigit():\n",
    "\t\t\tprint('# Overriding END for template region b_t%d with %f: ' % (region_index,float(end_override)))\n",
    "\t\t\tregions = set_baseline_region(regions, region_index, end_override=float(end_override))\n",
    "\t\telse:\n",
    "\t\t\tprint('# Skipping region... ')\n",
    "\n",
    "\tlc_info['baseline_rev_ix'] = np.concatenate([regions['b_t0'],regions['b_t1'],regions['b_t2']])\n",
    "\tlc_info['durings_rev_ix'] = AnotB(lc_info['lc'].getindices(),lc_info['baseline_rev_ix'])\n",
    "\n",
    "\treturn regions\n",
    "\n",
    "# correct control light curves for atlas template changes by \n",
    "# getting median of same baseline regions as SN, then applying to entire region\n",
    "def controls_correct_for_template(control_index, regions, region_index):\n",
    "    b_goodx2_ix = controls[control_index].ix_inrange(colnames=['chi/N'], uplim=5)\n",
    "    lowlim = lc_info['lc'].t.loc[regions[f'b_t{region_index}'][0], 'MJD']\n",
    "    uplim = lc_info['lc'].t.loc[regions[f'b_t{region_index}'][-1], 'MJD']\n",
    "    b_region_ix = controls[control_index].ix_inrange(colnames=['MJD'], lowlim=lowlim, uplim=uplim, exclude_uplim=True)\n",
    "    \n",
    "    if len(b_region_ix) > 0:\n",
    "        print(f'## Adjusting for template change in region b_t{region_index} from {lowlim:0.2f}-{uplim:0.2f}...')\n",
    "\t\t\t\t\t\t\n",
    "        if len(AandB(b_region_ix,b_goodx2_ix)) > 0:\n",
    "            median = np.median(controls[control_index].t.loc[AandB(b_region_ix,b_goodx2_ix),'uJy'])\n",
    "            print('### Median of measurements with chi-square ≤ 5 before correction: ', median)\n",
    "        else:\n",
    "            median = np.median(controls[control_index].t.loc[b_region_ix,'uJy'])\n",
    "            print('### Median of measurements with chi-square ≤ 5 before correction: ', median)\n",
    "\n",
    "        lowlim = lc_info['lc'].t.loc[regions[f't{region_index}'][0], 'MJD']\n",
    "        uplim = lc_info['lc'].t.loc[regions[f't{region_index}'][-1], 'MJD']\n",
    "        t_region_i = controls[control_index].ix_inrange(colnames=['MJD'], lowlim=lowlim, uplim=uplim, exclude_uplim=True)\n",
    "\n",
    "        print(f'### Subtracting median {median:0.1f} uJy from light curve flux due to potential flux in the template...')\n",
    "        controls[control_index].t.loc[t_region_i,'uJy'] -= median\n",
    "        print(f'### Median of measurements after correction: {np.median(controls[control_index].t.loc[b_region_ix, \"uJy\"])}')\n",
    "    else:\n",
    "        print(f'### No valid region for baseline region {region_index}, skipping...')\n",
    "\n",
    "def correct_for_template(regions, proceed):\n",
    "    if True in proceed.values():\n",
    "        f.write('\\n\\n## Correcting for ATLAS reference template changes')\n",
    "\n",
    "    b_goodx2_ix = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=5,indices=lc_info['baseline_rev_ix'])\n",
    "    b_badx2_ix = AnotB(lc_info['baseline_rev_ix'],b_goodx2_ix)\n",
    "\n",
    "    for region_index in range(0,3):\n",
    "        if proceed[region_index]:\n",
    "            region_ix = regions['b_t%d'%region_index]\n",
    "            if len(region_ix) > 0:\n",
    "                print('\\nAdjusting for template change in baseline region %d from %0.2f-%0.2f ' % (region_index, lc_info['lc'].t.loc[region_ix[0],'MJD'], lc_info['lc'].t.loc[region_ix[-1],'MJD']))\n",
    "                if len(AandB(region_ix,b_goodx2_ix)) > 0:\n",
    "                    median = np.median(lc_info['lc'].t.loc[AandB(region_ix,b_goodx2_ix),'uJy'])\n",
    "                    print('# Median of baseline measurements with chi-square ≤ 5 before correction: ', median)\n",
    "                else:\n",
    "                    median = np.median(lc_info['lc'].t.loc[region_ix,'uJy'])\n",
    "                    print('# Median of baseline measurements before correction: ', median)\n",
    "                \n",
    "                print(f'# Subtracting median {median:0.1f} uJy from light curve flux due to potential flux in the template...')\n",
    "                lc_info['lc'].t.loc[regions['t%d'%region_index],'uJy'] -= median\n",
    "                print('# Median of baseline measurements after correction: ', np.median(lc_info['lc'].t.loc[region_ix,'uJy']))\n",
    "                f.write(f'\\nCorrection applied to baseline region {region_index:d}: {median} uJy subtracted')\n",
    "\n",
    "                if load_controls:\n",
    "                    print(f'Correcting control light curves for potential flux in template...')\n",
    "                    for control_index in range(1, Ncontrols+1):\n",
    "                        print(f'# Control index: {control_index}')\n",
    "                        controls_correct_for_template(control_index, regions, region_index)\n",
    "            else:\n",
    "                print('No baseline region for region b_t%d, skipping... ' % region_index)\n",
    "\n",
    "def get_weighted_mean(lc, indices=None):\n",
    "    lc.calcaverage_sigmacutloop('uJy', indices=indices, Nsigma=3.0, median_firstiteration=True, verbose=1)\n",
    "    return lc.statparams['mean'] \n",
    "\n",
    "def get_typical_uncertainty(lc, indices=None):\n",
    "    if indices is None:\n",
    "        return np.median(lc.t['duJy'])\n",
    "    else:\n",
    "        return np.median(lc.t.loc[indices, 'duJy'])\n",
    "\n",
    "def proceed_check(regions):\n",
    "    # calculate weighted mean with 3-sigma clipping for each template region\n",
    "    # which should be below 2 * typical uncertainty (the median duJy of same region)\n",
    "    # if smaller, then print; else, plots\n",
    "    proceed = {}\n",
    "    for region_index in range(0,3):\n",
    "        region_ix = regions['b_t%d'%region_index]\n",
    "        print(f'Baseline region {region_index:d}')\n",
    "\n",
    "        weighted_mean = get_weighted_mean(lc_info['lc'], indices=region_ix)\n",
    "        typical_uncertainty = get_typical_uncertainty(lc_info['lc'], indices=region_ix) #np.median(lc_info['lc'].t.loc[region_ix, 'duJy'])\n",
    "        print(f'# Weighted mean flux (uJy) obtained using 3σ cut: {weighted_mean:0.2f}\\n# Typical uncertainty (duJy) obtained by taking median: {typical_uncertainty:0.2f}')\n",
    "\n",
    "        if weighted_mean > 2*typical_uncertainty:\n",
    "            answer = input(f'# WARNING: weighted mean flux > 2 * typical uncertainty! Template change correction strongly recommended for this region. Proceed? (y/n)')\n",
    "            if answer == 'y':\n",
    "                print('# Proceeding') \n",
    "                proceed[region_index] = True\n",
    "            else:\n",
    "                print('# Skipping')\n",
    "                proceed[region_index] = False\n",
    "        else:\n",
    "            print(f'# Weighted mean flux <= typical uncertainty. Proceeding with template change correction in this region')\n",
    "            proceed[region_index] = True\n",
    "\n",
    "    return proceed\n",
    "\n",
    "regions = get_baseline_regions(Ndays_min)\n",
    "\n",
    "# plot lc before correction for template\n",
    "if plot:\n",
    "    limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "    plot_all_lcs(add2title='\\nbefore correcting for template changes', templates=True, limits=limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, manually override baseline region endpoints; then correct for template changes\n",
    "\n",
    "# user can manually override baseline region endpoints\n",
    "if input('Override baseline region endpoint(s)? (y/n)') == 'y':\n",
    "    regions = set_baseline_regions(regions)\n",
    "\n",
    "# decide whether or not to correct\n",
    "proceed = proceed_check(regions)\n",
    "print(proceed)\n",
    "\n",
    "correct_for_template(regions, proceed)\n",
    "drop_extra_columns('lc') \n",
    "\n",
    "# plot lc after correction for template\n",
    "if plot:\n",
    "    limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "    plot_all_lcs(add2title='\\nafter correcting for template changes', templates=True, limits=limits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Static uncertainty cut\n",
    "\n",
    "The following uncertainty cut implements a static cut that applies the same way to each light curve. The purpose of this cut is to identify and clean out the most egregious outliers with large uncertainties and small chi-square values not cut in the dynamic chi-square cut. The default value of this cut (160) was determined after calculating the typical uncertainty of bright stars just below the saturation limit. \n",
    "\n",
    "WARNING: **if the SN is particularly bright, you may want to increase the value and rerun the cut**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may change the following static uncertainty cut value to your liking;\n",
    "# however, the default value is set to 160.\n",
    "lc_info['uncertainty_cut'] = 160\n",
    "\n",
    "# Plot the light curve before and after the applied uncertainty cut?:\n",
    "plot = False\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the uncertainty cut plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart 'Mask' column and update with uncertainty cut flag\n",
    "\n",
    "def update_mask_col(lc, flag, indices):\n",
    "    if len(indices) > 1:\n",
    "        flag_arr = np.full(lc.loc[indices,'Mask'].shape, flag)\n",
    "        lc.loc[indices,'Mask'] = np.bitwise_or(lc.loc[indices,'Mask'], flag_arr)\n",
    "    elif len(indices) == 1:\n",
    "        lc.loc[indices[0],'Mask'] = int(lc.loc[indices[0],'Mask']) | flag\n",
    "    else:\n",
    "        print('WARNING: must pass at least 1 index to update_mask_col()! No indices masked...')\n",
    "\n",
    "def plot_cut_lc(lc_type, mask, dflux_colname, add2title=None, limits=None):\n",
    "    good_ix = lc_info[lc_type].ix_unmasked('Mask',maskval=mask)\n",
    "    bad_ix = AnotB(lc_info[lc_type].getindices(),good_ix)\n",
    "\n",
    "    fig, (cut, clean) = plt.subplots(1, 2, figsize=(16, 6), tight_layout=True)\n",
    "    title = 'SN %s %s-band' % (lc_info['tnsname'], lc_info['filter'])\n",
    "    if lc_type == 'avglc':\n",
    "        title += ', averaged'\n",
    "    if not(add2title is None):\n",
    "        title += ', '+add2title\n",
    "    plt.suptitle(title, fontsize=19, y=1)\n",
    "\n",
    "    color = 'orange' if lc_info['filter'] == 'o' else 'cyan'\n",
    "    \n",
    "    # set x and y limits\n",
    "    limits = set_xylimits(limits, lc_type=lc_type)\n",
    "    if not(limits is None):\n",
    "        cut.set_xlim(limits[0],limits[1])\n",
    "        cut.set_ylim(limits[2],limits[3])\n",
    "        clean.set_xlim(limits[0],limits[1])\n",
    "        clean.set_ylim(limits[2],limits[3])\n",
    "    \"\"\"if ylim_lower is None: ylim_lower = -2000\n",
    "    if ylim_upper is None: \n",
    "        afterdiscdate_i = lc_info[lc_type].ix_inrange(colnames=['MJD'],lowlim=lc_info['discdate']) if lc_type == 'avglc' else lc_info['duringsn_ix']\n",
    "        ylim_upper = 3 * get_xth_percentile_flux(lc_type, 95, afterdiscdate_i)\n",
    "    if xlim_lower is None: xlim_lower = lc_info['discdate'] - 200\n",
    "    if xlim_upper is None: xlim_upper = lc_info['discdate'] + 800\n",
    "    cut.set_ylim(ylim_lower, ylim_upper)\n",
    "    cut.set_xlim(xlim_lower,xlim_upper)\n",
    "    clean.set_ylim(ylim_lower, ylim_upper)\n",
    "    clean.set_xlim(xlim_lower,xlim_upper)\"\"\"\n",
    "\n",
    "    cut.errorbar(lc_info[lc_type].t.loc[good_ix,'MJD'], lc_info[lc_type].t.loc[good_ix,'uJy'], yerr=lc_info[lc_type].t.loc[good_ix,dflux_colname], fmt='none',ecolor=color,elinewidth=1,c=color)\n",
    "    cut.scatter(lc_info[lc_type].t.loc[good_ix,'MJD'], lc_info[lc_type].t.loc[good_ix,'uJy'], s=50,color=color,marker='o',label='Kept measurements')\n",
    "    cut.errorbar(lc_info[lc_type].t.loc[bad_ix,'MJD'], lc_info[lc_type].t.loc[bad_ix,'uJy'], yerr=lc_info[lc_type].t.loc[bad_ix,dflux_colname], fmt='none',mfc='white',ecolor=color,elinewidth=1,c=color)\n",
    "    cut.scatter(lc_info[lc_type].t.loc[bad_ix,'MJD'], lc_info[lc_type].t.loc[bad_ix,'uJy'], s=50,facecolors='white',edgecolors=color,marker='o',label='Cut measurements')\n",
    "    cut.set_title('All measurements')\n",
    "    cut.axhline(linewidth=1,color='k')\n",
    "    cut.set_xlabel('MJD')\n",
    "    cut.set_ylabel('Flux (uJy)')\n",
    "\n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0),ncol=2)\n",
    "\n",
    "    clean.errorbar(lc_info[lc_type].t.loc[good_ix,'MJD'], lc_info[lc_type].t.loc[good_ix,'uJy'], yerr=lc_info[lc_type].t.loc[good_ix,dflux_colname], fmt='none',ecolor=color,elinewidth=1,c=color)\n",
    "    clean.scatter(lc_info[lc_type].t.loc[good_ix,'MJD'], lc_info[lc_type].t.loc[good_ix,'uJy'], s=50,color=color,marker='o',label='Kept measurements')\n",
    "    clean.set_title('Kept measurements only')\n",
    "    clean.axhline(linewidth=1,color='k')\n",
    "    clean.set_xlabel('MJD')\n",
    "    clean.set_ylabel('Flux (uJy)')\n",
    "    clean.set_ylim(ylim_lower, ylim_upper)\n",
    "\n",
    "def print_statistics(num_cut, percent_cut, flag, add2title=None):\n",
    "    print(f'\\nNumber of cut measurements: {num_cut:d}\\nPercent of total measurements cut: {percent_cut:0.2f}%')\n",
    "    if percent_cut > 10:\n",
    "        print(f'WARNING: percent of total measurements cut is greater than 10%. Plotting...')\n",
    "    if plot or percent_cut > 10:\n",
    "        limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "        plot_cut_lc('lc', flag, dflux_colnames[0], add2title=add2title, limits=limits)\n",
    "\n",
    "# remove old mask column\n",
    "if 'Mask' in lc_info['lc'].t.columns: \n",
    "    print('Removing old \\'Mask\\' column...')\n",
    "    lc_info['lc'].t.drop(columns=['Mask'],inplace=True)\n",
    "\n",
    "# create new mask column and update it with uncertainty cut\n",
    "print(f'Applying uncertainty cut of {lc_info[\"uncertainty_cut\"]:0.2f}...')\n",
    "lc_info['lc'].t['Mask'] = 0\n",
    "kept_ix = lc_info['lc'].ix_inrange(colnames=['duJy'],uplim=lc_info['uncertainty_cut'])\n",
    "cut_ix = AnotB(lc_info['lc'].getindices(), kept_ix)\n",
    "update_mask_col(lc_info['lc'].t, flag_uncertainty, cut_ix)\n",
    "print('Success')\n",
    "\n",
    "num_cut = len(cut_ix)\n",
    "percent_cut = 100 * num_cut/len(lc_info['lc'].t)\n",
    "print_statistics(num_cut, percent_cut, flag_uncertainty, add2title='uncertainty cut')\n",
    "f.write(f'\\n\\n## Uncertainty cut\\nNumber of cut measurements: {num_cut:d}\\nPercent of total measurements cut: {percent_cut:0.2f}%\\nHex value in \"Mask\" column: 0x2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Estimating true uncertainties\n",
    "\n",
    "This section attempts to account for an extra noise source in the data by estimating the true typical uncertainty, deriving the additional systematic uncertainty, and lastly applying this extra noise to a new uncertainty column. This new uncertainty column will be used in the cuts following this section.\n",
    "\n",
    "Here is the exact procedure we use:\n",
    "1. Keep the previously applied uncertainty cut and apply a preliminary chi-square cut at 20 (default value). Filter out any measurements flagged by these two cuts.\n",
    "2. Calculate the true typical uncertainty $\\text{sigma\\_true\\_typical}$ by taking a 3σ cut of the unflagged baseline flux and getting the standard deviation.\n",
    "3. If $\\text{sigma\\_true\\_typical}$ is 10%+ greater than the median uncertainty of the unflagged baseline flux, $\\text{median}(∂µJy)$, proceed with estimating the extra noise to add. Otherwise, skip this procedure. \n",
    "4. Calculate the extra noise source using the following formula, where the median uncertainty, $\\text{median}(∂µJy)$, is taken from the unflagged baseline flux:\n",
    "    - $\\text{sigma\\_extra}^2 = \\text{sigma\\_true\\_typical}^2 - \\text{sigma\\_poisson}^2 = \\text{sigma\\_true\\_typical}^2 - \\text{median}(∂µJy)^2 $\n",
    "5. Apply the extra noise source to the existing uncertainty using the following formula:\n",
    "    - $\\text{new }∂µJy = \\sqrt{(\\text{old }∂µJy)^2 + \\text{sigma\\_extra}^2}$\n",
    "6. Repeat steps 1-5 for each control light curve. For cuts following this procedure, use the new uncertainty column with the extra noise added instead of the old uncertainty column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter a preliminary chi-square cut (keep at a high number):\n",
    "prelim_x2_cut = 20\n",
    "\n",
    "# Plot the light curve before and after estimating true uncertainties?:\n",
    "plot = True\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate true uncertainties and create revised 'duJy_new' column; store preferred column names in dflux_colnames list\n",
    "\n",
    "def get_median_dflux(lc, b_clean_ix):\n",
    "    return np.median(lc.t.loc[b_clean_ix, 'duJy'])\n",
    "\n",
    "def get_sigma_true_typical(lc, b_clean_ix):\n",
    "    lc.calcaverage_sigmacutloop('uJy', indices=b_clean_ix, Nsigma=3.0, median_firstiteration=True, verbose=1)\n",
    "    #print(lc.statparams)\n",
    "    return lc.statparams['stdev']\n",
    "\n",
    "def get_sigma_extra(sigma_true_typical, median_dflux):\n",
    "    sigma_extra = np.sqrt(sigma_true_typical*sigma_true_typical - median_dflux)\n",
    "    #print('# Sigma extra calculated: %0.4f' % sigma_extra)\n",
    "    return sigma_extra\n",
    "\n",
    "def add_sigma_extra(lc, sigma_extra):\n",
    "    print(f'# Adding sigma_extra {sigma_extra:0.4f} to new duJy column...')\n",
    "    lc.t['duJy_new'] = np.sqrt(lc.t['duJy']*lc.t['duJy'] + sigma_extra*sigma_extra)\n",
    "\n",
    "def proceed_check(prelim_x2_cut, baseline_ix=None):\n",
    "    print('Checking to see if true uncertainties estimation is necessary...')\n",
    "    lc = lc_info['lc']\n",
    "    if baseline_ix is None:\n",
    "        baseline_ix = lc_info['baseline_rev_ix'] \n",
    "    clean_ix = AandB(lc.ix_unmasked('Mask',maskval=flag_uncertainty), lc.ix_inrange(colnames=['chi/N'],uplim=prelim_x2_cut,exclude_uplim=True))\n",
    "    b_clean_ix = AandB(baseline_ix, clean_ix)\n",
    "\n",
    "    sigma_true_typical = get_sigma_true_typical(lc, b_clean_ix)\n",
    "    median_dflux = get_median_dflux(lc, b_clean_ix)\n",
    "    print(f'Median uncertainty of baseline flux: {median_dflux:0.2f}\\nTrue typical uncertainty of baseline flux: {sigma_true_typical:0.2f}')\n",
    "    \n",
    "    percent_greater = 100 * ((sigma_true_typical - median_dflux)/median_dflux)\n",
    "    if percent_greater >= 10:\n",
    "        print(f'WARNING: True typical uncertainty is {percent_greater:0.2f}% greater than median uncertainty of baseline flux. True uncertainties estimation recommended')\n",
    "        answer = input('Proceed with true uncertainties estimation? (y/n):')\n",
    "        if answer == 'y':\n",
    "            print('Proceeding...') \n",
    "            return True\n",
    "        else:\n",
    "            print('Skipping procedure')\n",
    "            return False\n",
    "    print(f'True typical uncertainty is {percent_greater:0.2f}% greater than median uncertainty of baseline flux--no estimation needed!')\n",
    "    return False\n",
    "\n",
    "def estimate_true_uncertainties(prelim_x2_cut, control_index, baseline_ix=None):\n",
    "    clean_ix = AandB(lc_info['lc'].ix_unmasked('Mask',maskval=flag_uncertainty), lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=prelim_x2_cut,exclude_uplim=True))\n",
    "    b_clean_ix = AandB(lc_info['baseline_rev_ix'], clean_ix)\n",
    "\n",
    "    if control_index == 0:\n",
    "        print('\\nEstimating true uncertainties for SN light curve...')\n",
    "        lc = lc_info['lc']\n",
    "        if baseline_ix is None:\n",
    "            baseline_ix = lc_info['baseline_rev_ix'] \n",
    "        clean_ix = AandB(lc.ix_unmasked('Mask',maskval=flag_uncertainty), lc.ix_inrange(colnames=['chi/N'],uplim=prelim_x2_cut,exclude_uplim=True))\n",
    "        b_clean_ix = AandB(baseline_ix, clean_ix)\n",
    "    else:\n",
    "        print(f'\\nEstimating true uncertainties for control light curve {control_index:03d}...')\n",
    "        lc = controls[control_index]\n",
    "        b_clean_ix =  AandB(lc.ix_unmasked('Mask',maskval=flag_uncertainty), lc.ix_inrange(colnames=['chi/N'],uplim=prelim_x2_cut,exclude_uplim=True))\n",
    "\n",
    "    sigma_true_typical = get_sigma_true_typical(lc, b_clean_ix)\n",
    "    median_dflux = get_median_dflux(lc, b_clean_ix)\n",
    "    print(f'# Median uncertainty: {median_dflux:0.2f}; true typical uncertainty: {sigma_true_typical:0.2f}')\n",
    "\n",
    "    # use new uncertainty column from now on\n",
    "    dflux_colnames[control_index] = 'duJy_new'\n",
    "\n",
    "    sigma_extra = get_sigma_extra(sigma_true_typical, median_dflux)\n",
    "    add_sigma_extra(lc, sigma_extra)\n",
    "    if control_index == 0:\n",
    "        f.write(f'\\n\\n## Estimating true uncertainties\\nMedian uncertainty: {median_dflux:0.2f} duJy\\nTrue typical uncertainty: {sigma_true_typical:0.2f} duJy\\nExtra noise added (in \"duJy_new\" column): {sigma_extra:0.2f} duJy')\n",
    "\n",
    "    # recalculate uJy/duJy column\n",
    "    print('# Recalculating uJy/duJy column using duJy_new as the uncertainties...')\n",
    "    lc_info['lc'].t['uJy/duJy'] = lc_info['lc'].t['uJy']/lc_info['lc'].t['duJy_new']\n",
    "    lc_info['lc'].t = lc_info['lc'].t.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def plot_trueuncerts(baseline_ix=None, duringsn_ix=None, limits=None):\n",
    "    color = 'orange' if lc_info[\"filter\"] == 'o' else 'cyan'\n",
    "\n",
    "    if baseline_ix is None:\n",
    "        baseline_ix = lc_info['baseline_rev_ix']\n",
    "    if duringsn_ix is None:\n",
    "        duringsn_ix = lc_info['duringsn_ix']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, constrained_layout=True)\n",
    "    fig.set_figwidth(12)\n",
    "    fig.set_figheight(8)\n",
    "    \n",
    "    ax1.set_title(f'SN {lc_info[\"tnsname\"]} {lc_info[\"filter\"]}-band flux\\nbefore estimating true uncertainties')\n",
    "    ax1.axvline(x=tchange1, color='magenta', label='ATLAS template change')\n",
    "    ax1.axvline(x=tchange2, color='magenta')\n",
    "    ax1.set_xlim(xlim_lower,xlim_upper)\n",
    "    ax1.set_ylim(ylim_lower,ylim_upper)\n",
    "    ax1.get_xaxis().set_ticks([])\n",
    "    ax1.set_ylabel('Flux (µJy)')\n",
    "    ax1.errorbar(lc_info['lc'].t.loc[baseline_ix,'MJD'], lc_info['lc'].t.loc[baseline_ix,'uJy'], yerr=lc_info['lc'].t.loc[baseline_ix, 'duJy'], fmt='none',ecolor=color, elinewidth=1, c=color, zorder=10)\n",
    "    ax1.scatter(lc_info['lc'].t.loc[baseline_ix,'MJD'],lc_info['lc'].t.loc[baseline_ix,'uJy'], s=45, color=color, marker='o', zorder=10, label='Baseline')\n",
    "    ax1.errorbar(lc_info['lc'].t.loc[duringsn_ix,'MJD'], lc_info['lc'].t.loc[duringsn_ix,'uJy'], lc_info['lc'].t.loc[duringsn_ix, 'duJy'], fmt='none', ecolor='red', elinewidth=1, c='red', zorder=20)\n",
    "    ax1.scatter(lc_info['lc'].t.loc[duringsn_ix,'MJD'], lc_info['lc'].t.loc[duringsn_ix,'uJy'], s=45, color='red', marker='o', zorder=20, label='During SN')\n",
    "    ax1.axhline(linewidth=1, color='k', zorder=30)\n",
    "    ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    ax2.set_title('after estimating true uncertainties')\n",
    "    ax2.axvline(x=tchange1, color='magenta', label='ATLAS template change')\n",
    "    ax2.axvline(x=tchange2, color='magenta')\n",
    "    ax2.set_xlim(xlim_lower,xlim_upper)\n",
    "    ax2.set_ylim(ylim_lower,ylim_upper)\n",
    "    ax2.set_ylabel('Flux (µJy)')\n",
    "    ax2.set_xlabel('MJD')\n",
    "    ax2.errorbar(lc_info['lc'].t.loc[baseline_ix,'MJD'], lc_info['lc'].t.loc[baseline_ix,'uJy'], yerr=lc_info['lc'].t.loc[baseline_ix, 'duJy_new'], fmt='none',ecolor=color, elinewidth=1, c=color, zorder=10)\n",
    "    ax2.scatter(lc_info['lc'].t.loc[baseline_ix,'MJD'],lc_info['lc'].t.loc[baseline_ix,'uJy'], s=45, color=color, marker='o', zorder=10, label='Baseline')\n",
    "    ax2.errorbar(lc_info['lc'].t.loc[duringsn_ix,'MJD'], lc_info['lc'].t.loc[duringsn_ix,'uJy'], lc_info['lc'].t.loc[duringsn_ix, 'duJy_new'], fmt='none', ecolor='red', elinewidth=1, c='red', zorder=20)\n",
    "    ax2.scatter(lc_info['lc'].t.loc[duringsn_ix,'MJD'], lc_info['lc'].t.loc[duringsn_ix,'uJy'], s=45, color='red', marker='o', zorder=20, label='During SN')\n",
    "    ax2.axhline(linewidth=1, color='k', zorder=30)\n",
    "    ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    # set x and y limits\n",
    "    limits = set_xylimits(limits)\n",
    "    if not(limits is None):\n",
    "        ax1.set_xlim(limits[0],limits[1])\n",
    "        ax1.set_ylim(limits[2],limits[3])\n",
    "        ax2.set_xlim(limits[0],limits[1])\n",
    "        ax2.set_ylim(limits[2],limits[3])\n",
    "\n",
    "check = proceed_check(prelim_x2_cut, baseline_ix=lc_info['baseline_rev_ix'])\n",
    "\n",
    "if check:\n",
    "    for control_index in range(Ncontrols+1):\n",
    "        estimate_true_uncertainties(prelim_x2_cut, control_index)\n",
    "        if control_index == 0 and plot:\n",
    "            limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "            plot_trueuncerts(limits=limits)\n",
    "    print('\\nSuccess')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Dynamic chi-square cut\n",
    "\n",
    "### 5a) Plot the flux/dflux and chi-square distributions\n",
    "\n",
    "The following two histograms display the flux/dflux and chi-square distributions of the target SN. Both histograms show probability density so as to ease comparison between the groups plotted within each histogram.\n",
    "\n",
    "- The first histogram focuses on the baseline flux/dflux (µJy/dµJy) measurements, where we can expect the flux to equal 0. In orange, we plot flux/dflux (µJy/dµJy) measurements with a chi-square value less than or equal to `x2bound`, which is currently set to 5 below; in blue, we plot flux/dflux (µJy/dµJy) measurements with a chi-square value greater than `x2bound`. \n",
    "- The second histogram focuses on the baseline chi-square measurements. In green, we plot chi-square measurements with an abs(µJy/dµJy) value less than or equal to `stnbound`, which is currently set to 3 below; in red, we plot chi-square measurements with an abs(µJy/dµJy) value greater than `stnbound`. \n",
    "\n",
    "Ideally, all measurements with a chi-square value less than or equal to `x2bound` should have an abs(µJy/dµJy) value less than or equal to `stnbound`, and measurements with a chi-square value greater than `x2bound` should have an abs(µJy/dµJy) value greater than `stnbound`. Our goal is to separate good measurements from bad measurements using a chi-square cut; in order for our cut to be effective, these histograms should hopefully showcase this relation between the target SN's flux/dflux and chi-square measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the bound that should separate a good chi-square measurement from a bad one:\n",
    "x2bound = 5.0\n",
    "\n",
    "# Enter the bound that should separate a good abs(flux/dflux) measurement from a bad one:\n",
    "stnbound = 3.0\n",
    "\n",
    "# Optionally, manually enter the histograms' x limits here:\n",
    "# flux/dflux histogram x limits:\n",
    "fdf_xlim_lower = None \n",
    "fdf_xlim_upper = None\n",
    "# chi-square histogram x limits:\n",
    "x2_xlim_lower = None\n",
    "x2_xlim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot flux/dflux and chi-square distribution histograms\n",
    "\n",
    "def plot_hists(x2bound, stnbound, fdf_xlim_lower=None, fdf_xlim_upper=None, x2_xlim_lower=None, x2_xlim_upper=None, baseline_ix=None):\n",
    "    if baseline_ix is None:\n",
    "        baseline_ix = lc_info['baseline_rev_ix']\n",
    "\n",
    "    b_goodstn_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'], lowlim=-stnbound, uplim=stnbound, indices=baseline_ix)\n",
    "    b_badstn_i = AnotB(baseline_ix, b_goodstn_i)\n",
    "    b_goodx2_i = lc_info['lc'].ix_inrange(colnames=['chi/N'], uplim=x2bound, indices=baseline_ix)\n",
    "    b_badx2_i = AnotB(baseline_ix, b_goodx2_i)\n",
    "\n",
    "    fig, (stn, x2) = plt.subplots(1, 2, figsize=(10, 6.5), tight_layout=True)\n",
    "    plt.suptitle('SN %s %s-band, baseline only' % (lc_info['tnsname'], lc_info['filter']), fontsize=17, y=1)\n",
    "\n",
    "    stn.set_title('µJy/dµJy distribution')\n",
    "    stn.set_xlabel('µJy/dµJy')\n",
    "    stn.spines.right.set_visible(False)\n",
    "    stn.spines.top.set_visible(False)\n",
    "    orange = mpatches.Patch(color='orange', label='Data with chi-square<%0.2f' % x2bound)\n",
    "    blue = mpatches.Patch(color='blue', label='Data with chi-square≥%0.2f' % x2bound)\n",
    "    stn.legend(handles=[orange, blue], loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=1)\n",
    "    if len(baseline_ix)>0: \n",
    "        if fdf_xlim_lower is None: \n",
    "            fdf_xlim_lower = min(lc_info['lc'].t.loc[baseline_ix, 'uJy/duJy'])\n",
    "        if fdf_xlim_upper is None: \n",
    "            fdf_xlim_upper = max(lc_info['lc'].t.loc[baseline_ix, 'uJy/duJy'])\n",
    "        stn.hist(lc_info['lc'].t.loc[b_goodx2_i, 'uJy/duJy'], bins=30, color='orange', alpha=0.5, range=(fdf_xlim_lower,fdf_xlim_upper), density=True)\n",
    "        stn.hist(lc_info['lc'].t.loc[b_badx2_i, 'uJy/duJy'], bins=30, color='blue', alpha=0.5, range=(fdf_xlim_lower,fdf_xlim_upper), density=True)\n",
    "    else:\n",
    "        stn.hist(lc_info['lc'].t.loc[b_goodx2_i, 'uJy/duJy'], bins=30, color='orange', alpha=0.5, density=True)\n",
    "        stn.hist(lc_info['lc'].t.loc[b_badx2_i, 'uJy/duJy'], bins=30, color='blue', alpha=0.5, density=True)\n",
    "\n",
    "    x2.set_title('Chi-square distribution')\n",
    "    x2.set_xlabel('Chi-square')\n",
    "    x2.spines.right.set_visible(False)\n",
    "    x2.spines.top.set_visible(False)\n",
    "    red = mpatches.Patch(color='green', label='Data with µJy/dµJy<%0.2f' % stnbound)\n",
    "    green = mpatches.Patch(color='red', label='Data with µJy/dµJy≥%0.2f' % stnbound)\n",
    "    x2.legend(handles=[red, green], loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=1)\n",
    "    if len(baseline_ix)>0:\n",
    "        if x2_xlim_lower is None: \n",
    "            x2_xlim_lower = min(lc_info['lc'].t.loc[baseline_ix, 'chi/N'])\n",
    "        if x2_xlim_upper is None: \n",
    "            x2_xlim_upper = max(lc_info['lc'].t.loc[baseline_ix, 'chi/N'])\n",
    "        x2.hist(lc_info['lc'].t.loc[b_goodstn_i, 'chi/N'], bins=30, color='green', alpha=0.5, range=(x2_xlim_lower,x2_xlim_upper), density=True)\n",
    "        x2.hist(lc_info['lc'].t.loc[b_badstn_i, 'chi/N'], bins=30, color='red', alpha=0.5, range=(x2_xlim_lower,x2_xlim_upper), density=True)\n",
    "    else:\n",
    "        x2.hist(lc_info['lc'].t.loc[b_goodstn_i, 'chi/N'], bins=30, color='green', alpha=0.5, density=True)\n",
    "        x2.hist(lc_info['lc'].t.loc[b_badstn_i, 'chi/N'], bins=30, color='red', alpha=0.5, density=True)\n",
    "\n",
    "plot_hists(x2bound, stnbound, fdf_xlim_lower=fdf_xlim_lower, fdf_xlim_upper=fdf_xlim_upper, x2_xlim_lower=x2_xlim_lower, x2_xlim_upper=x2_xlim_upper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b) Calculate best chi-square cut based on contamination and loss\n",
    "\n",
    "The following cells use two factors, <strong>contamination</strong> and <strong>loss</strong>, to attempt to calculate an optimal PSF chi-square cut for the target SN, with flux/dflux as the deciding factor of what constitutes a good measurement vs. a bad measurement. We aim to separate good measurements from bad using the calculated chi-square cut by removing as much contamination as possible with the smallest loss possible. Since we can assume that the expected value of the baseline flux is 0, we look only at the baseline measurements before the SN occurs in order to determine the best chi-square cut for the SN itself.\n",
    "\n",
    "First, we decide what will determine a good measurement vs. a bad measurement using a factor outside of the chi-square values. Our chosen factor is the <strong>absolute value of flux (µJy) divided by dflux (dµJy)</strong>. The recommended boundary is a value of 3, such that any measurements with a value of abs(µJy/dµJy) less than or equal to 3 are regarded as \"good\" measurements, and any measurements with a value of abs(µJy/dµJy) greater than 3 are regarded as \"bad\" measurements. You can set this boundary to a different number by changing the value of `stn_cut` below.\n",
    "\n",
    "Next, we set the upper and lower bounds of our final chi-square cut. We start at a low value of 3 (which can be changed by setting the value of `cut_start` below) and end at 50 (this value is inclusive and can be changed by setting the value of `cut_stop` below) with a step size of 1 (`cut_step` below). <strong>For chi-square cuts falling on or between `cut_start` and `cut_stop` in increments of `cut_step`, we can begin to calculate contamination and loss percentages.</strong>\n",
    "\n",
    "We define contamination to be the number of bad kept measurements over the total number of kept measurements for that chi-square cut (<strong>contamination = Nbad,kept/Nkept</strong>). For our final chi-square cut, we can also set a limit on what maximum percent contamination we want to have--the recommended value is <strong>15%</strong> but can be changed by setting the value of `contam_lim` below.\n",
    "\n",
    "We define loss to be the number of good cut measurements over the total number of good measurements for that chi-square cut (<strong>loss = Ngood,cut/Ngood</strong>). For our final chi-square cut, we can also set a limit on what maximum percent loss we want to have--the recommended value is <strong>10%</strong> but can be changed by setting the value of `loss_lim` below.\n",
    "\n",
    "Finally, we define which limit (`contam_lim` or `loss_lim`) to prioritize in the event that an optimal chi-square cut fitting both limits is not found. The default prioritized limit is `loss_lim` but can be changed by setting the value of `lim_to_prioritize` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the abs(uJy/duJy) boundary that will determine a \"good\" measurement vs. \"bad\" measurement:\n",
    "stn_cut = 3\n",
    "\n",
    "# Enter the bounds for the final chi-square cut (minimum cut, maximum cut, and step):\n",
    "cut_start = 3 # this is inclusive\n",
    "cut_stop = 50 # this is inclusive\n",
    "cut_step = 1\n",
    "\n",
    "# Enter the contamination limit (contamination = Nbad,kept/Nkept must be <= contam_lim% \n",
    "# for the final chi-square cut):\n",
    "contam_lim = 15.0\n",
    "\n",
    "# Enter the loss limit (loss = Ngood,cut/Ngood must be >= loss_lim%\n",
    "# for the final chi-square cut):\n",
    "loss_lim = 10.0\n",
    "\n",
    "# Enter the limit to prioritize (must be 'loss_lim' or 'contam_lim') in the event that\n",
    "# one or both limits are not met:\n",
    "lim_to_prioritize = 'loss_lim'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section describes in detail how we determine the final chi-square cut using the given contamination and loss limits (feel free to skip).\n",
    "\n",
    "For each given limit (contamination and loss), we calculate a range of valid cuts whose contamination/loss percentage is less than that limit and then choose a single cut within that valid range. Then, we pass through a decision tree to determine which of the two suggested cuts to use using a variety of factors (including the user's selected `lim_to_prioritize`).\n",
    "\n",
    "When choosing the loss cut according to the loss percentage limit `loss_lim`:\n",
    "- <strong>If all loss percentages are below the limit</strong> `loss_lim`, all cuts falling on or between `cut_start` and `cut_stop` are valid.\n",
    "- <strong>If all loss percentages are above the limit</strong> `loss_lim`, a cut with the required loss percentage is not possible; therefore, any cuts with the smallest percentage of loss are valid.\n",
    "- <strong>Otherwise</strong>, the valid range of cuts includes any cuts with the loss percentage less than or equal to the limit `loss_lim`.\n",
    "- The chosen cut for this limit is the <strong>minimum cut</strong> within the stated valid range of cuts.\n",
    "\n",
    "When choosing the loss cut according to the contamination percentage limit `contam_lim`:\n",
    "- <strong>If all contamination percentages are below the limit</strong> `contam_lim`, all cuts falling on or between `cut_start` and `cut_stop` are valid.\n",
    "- <strong>If all contamination percentages are above the limit</strong> `contam_lim`, a cut with the required contamination percentage is not possible; therefore, any cuts with the smallest percentage of contamination are valid.\n",
    "- <strong>Otherwise</strong>, the valid range of cuts includes any cuts with the contamination percentage less than or equal to the limit `contam_lim`.\n",
    "- The chosen cut for this limit is the <strong>maximum cut</strong> within the stated valid range of cuts.\n",
    "\n",
    "After we have calculated two suggested cuts based on the loss and contamination percentage limits, we follow the decision tree in order to suggest a final cut:\n",
    "- If both loss and contamination cut percentages were chosen from a range that spanned from `cut_start` to `cut_stop`, we set the final cut to `cut_start`.\n",
    "- If one cut's percentage was chosen from a range that spanned from `cut_start` to `cut_stop` and the other cut's percentage was not, we set the final cut to the latter cut.\n",
    "- If both percentages were chosen from ranges that fell above their respective limits, we suggest reselecting either or both limits.\n",
    "- Otherwise, we take into account the user's prioritized limit `lim_to_prioritize`:\n",
    "    - If the loss cut is greater than the contamination cut, we set the final cut to whichever cut is associated with `lim_to_prioritize`.\n",
    "    - Otherwise, if `lim_to_prioritize` is set to `contam_lim`, we set the final cut to the loss cut, and if `lim_to_prioritize` is set to `loss_lim`, we set the final cut to the contamination cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the suggested best chi-square cut using contamination and loss\n",
    "\n",
    "def plot_lim_cuts(lim_cuts, contam_lim_cut, loss_lim_cut):\n",
    "    fig = plt.figure(figsize=(10,5), tight_layout=True)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.title('SN %s %s-band chi-square cut' % (lc_info['tnsname'],lc_info['filter']))\n",
    "\n",
    "    plt.axhline(linewidth=1,color='k')\n",
    "    plt.xlabel('Chi-square cut')\n",
    "    plt.ylabel('% of baseline measurements')\n",
    "\n",
    "    plt.axhline(loss_lim,linewidth=1,color='r',linestyle='--',label='Loss limit')\n",
    "    plt.plot(lim_cuts.t['PSF Chi-Square Cut'], lim_cuts.t['Ploss'],ms=5,color='r',marker='o',label='Loss')\n",
    "    plt.axvline(x=loss_lim_cut,color='r',label='Loss cut')\n",
    "    plt.axvspan(loss_lim_cut, cut_stop, alpha=0.2, color='r')\n",
    "\n",
    "    plt.axhline(contam_lim,linewidth=1,color='g',linestyle='--',label='Contamination limit')\n",
    "    plt.plot(lim_cuts.t['PSF Chi-Square Cut'], lim_cuts.t['Pcontamination'],ms=5,color='g',marker='o',label='Contamination')\n",
    "    plt.axvline(x=contam_lim_cut,color='g',label='Contamination cut')\n",
    "    plt.axvspan(cut_start, contam_lim_cut, alpha=0.2, color='g')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    #fig.savefig('000001.png',bbox_inches=\"tight\",dpi=200)\n",
    "\n",
    "def choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case):\n",
    "    # case 1 and 1: final_cut = 3\n",
    "    # case 1 and 2: take limit of case 2\n",
    "    # case 1 and 3: take limit of case 3\n",
    "    # case 2 and 2: print lims don't work\n",
    "    # case 2 and 3: choose_btwn_lim_cuts\n",
    "    # case 3 and 3: choose_btwn_lim_cuts\n",
    "\n",
    "    case1 = loss_case == 'below lim' or contam_case == 'below lim'\n",
    "    case2 = loss_case == 'above lim' or contam_case == 'above lim'\n",
    "    case3 = loss_case == 'crosses lim' or contam_case == 'crosses lim'\n",
    "\n",
    "    final_cut = None\n",
    "    if case1 and not case2 and not case3: # 1 and 1\n",
    "        print('Valid chi-square cut range from %0.2f to %0.2f! Setting to 3...' % (loss_lim_cut, contam_lim_cut))\n",
    "        final_cut = cut_start\n",
    "    elif case1: # 1\n",
    "        if case2: # and 2\n",
    "            if loss_case == 'above lim':\n",
    "                print('WARNING: contam_lim_cut <= %0.2f falls below limit %0.2f%%, but loss_lim_cut >= %0.2f falls above limit %0.2f%%! Setting to %0.2f...' % (contam_lim_cut, contam_lim, loss_lim_cut, loss_lim, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('WARNING: loss_lim_cut <= %0.2f falls below limit %0.2f%%, but contam_lim_cut >= %0.2f falls above limit %0.2f%%! Setting to %0.2f...' % (loss_lim_cut, loss_lim, contam_lim_cut, contam_lim, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "        else: # and 3\n",
    "            if loss_case == 'crosses lim':\n",
    "                print('Contam_lim_cut <= %0.2f falls below limit %0.2f%% and loss_lim_cut >= %0.2f crosses limit %0.2f%%, setting to %0.2f...' % (contam_lim_cut, contam_lim, loss_lim_cut, loss_lim, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Loss_lim_cut <= %0.2f falls below limit %0.2f%% and contam_lim_cut >= %0.2f crosses limit %0.2f%%, setting to %0.2f...' % (loss_lim_cut, loss_lim, contam_lim_cut, contam_lim, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    elif case2 and not case3: # 2 and 2\n",
    "        print('ERROR: chi-square loss_lim_cut >= %0.2f and contam_lim_cut <= %0.2f both fall above limits %0.2f%% and %0.2f%%! Try setting less strict limits. Setting final cut to nan.' % (loss_lim_cut, contam_lim_cut, loss_lim, contam_lim))\n",
    "        final_cut = np.nan\n",
    "    else: # 2 and 3 or 3 and 3\n",
    "        if loss_lim_cut > contam_lim_cut:\n",
    "            print('WARNING: chi-square loss_lim_cut >= %0.2f and contam_lim_cut <= %0.2f do not overlap! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %0.2f...' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "        else:\n",
    "            print('Valid chi-square cut range from %0.2f to %0.2f! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    return final_cut\n",
    "\n",
    "def get_lim_cuts_data(cut, colname, baseline_ix=None):\n",
    "    if baseline_ix is None:\n",
    "        baseline_ix = lc_info['baseline_rev_ix'] \n",
    "\n",
    "    b_good_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=baseline_ix)\n",
    "    b_bad_i = AnotB(baseline_ix, b_good_i)\n",
    "    b_kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=cut,indices=baseline_ix)\n",
    "    b_cut_i = AnotB(baseline_ix, b_kept_i)\n",
    "\n",
    "    lc_info['%s_Ngood' % colname] = len(b_good_i)\n",
    "    lc_info['%s_Nbad' % colname] = len(b_bad_i)\n",
    "    lc_info['%s_Nkept' % colname] = len(b_kept_i)\n",
    "    lc_info['%s_Ncut' % colname] = len(b_cut_i)\n",
    "    lc_info['%s_Ngood,kept' % colname] = len(AandB(b_good_i,b_kept_i))\n",
    "    lc_info['%s_Ngood,cut' % colname] = len(AandB(b_good_i,b_cut_i))\n",
    "    lc_info['%s_Nbad,kept' % colname] = len(AandB(b_bad_i,b_kept_i))\n",
    "    lc_info['%s_Nbad,cut' % colname] = len(AandB(b_bad_i,b_cut_i))\n",
    "    lc_info['%s_Pgood,kept' % colname] = 100*len(AandB(b_good_i,b_kept_i))/len(baseline_ix)\n",
    "    lc_info['%s_Pgood,cut' % colname] = 100*len(AandB(b_good_i,b_cut_i))/len(baseline_ix)\n",
    "    lc_info['%s_Pbad,kept' % colname] = 100*len(AandB(b_bad_i,b_kept_i))/len(baseline_ix)\n",
    "    lc_info['%s_Pbad,cut' % colname] = 100*len(AandB(b_bad_i,b_cut_i))/len(baseline_ix)\n",
    "    lc_info['%s_Ngood,kept/Ngood' % colname] = 100*len(AandB(b_good_i,b_kept_i))/len(b_good_i)\n",
    "    lc_info['%s_Ploss' % colname] = 100*len(AandB(b_good_i,b_cut_i))/len(b_good_i)\n",
    "    lc_info['%s_Pcontamination' % colname] = 100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i)\n",
    "\n",
    "def get_lim_cuts(lim_cuts): \n",
    "    contam_lim_cut = None\n",
    "    loss_lim_cut = None\n",
    "    contam_case = None\n",
    "    loss_case = None\n",
    "\n",
    "    sortby_loss = lim_cuts.t.iloc[(lim_cuts.t['Ploss']).argsort()].reset_index()\n",
    "    min_loss = sortby_loss.loc[0,'Ploss']\n",
    "    max_loss = sortby_loss.loc[len(sortby_loss)-1,'Ploss']\n",
    "    # if all loss below lim, loss_lim_cut is min cut\n",
    "    if min_loss < loss_lim and max_loss < loss_lim:\n",
    "        loss_case = 'below lim'\n",
    "        loss_lim_cut = lim_cuts.t.loc[0,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all loss above lim, loss_lim_cut is min cut with min% loss\n",
    "        if min_loss > loss_lim and max_loss > loss_lim:\n",
    "            loss_case = 'above lim'\n",
    "            a = np.where(lim_cuts.t['Ploss'] == min_loss)[0]\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            loss_lim_cut = c.loc[0,'PSF Chi-Square Cut']\n",
    "        # else if loss crosses lim at some point, loss_lim_cut is min cut with max% loss <= loss_lim\n",
    "        else:\n",
    "            loss_case = 'crosses lim'\n",
    "            valid_cuts = sortby_loss[sortby_loss['Ploss'] <= loss_lim]\n",
    "            a = np.where(lim_cuts.t['Ploss'] == valid_cuts.loc[len(valid_cuts)-1,'Ploss'])[0]\n",
    "            # sort by cuts\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            # get midpoint of loss1 and loss2 (two points on either side of lim)\n",
    "            loss1_i = np.where(lim_cuts.t['PSF Chi-Square Cut'] == c.loc[0,'PSF Chi-Square Cut'])[0][0]\n",
    "            if lim_cuts.t.loc[loss1_i,'Ploss'] == loss_lim:\n",
    "                loss_lim_cut = lim_cuts.t.loc[loss1_i,'PSF Chi-Square Cut']\n",
    "            else:\n",
    "                loss2_i = loss1_i - 1\n",
    "                x = np.array([lim_cuts.t.loc[loss1_i,'PSF Chi-Square Cut'], lim_cuts.t.loc[loss2_i,'PSF Chi-Square Cut']])\n",
    "                contam_y = np.array([lim_cuts.t.loc[loss1_i,'Pcontamination'], lim_cuts.t.loc[loss2_i,'Pcontamination']])\n",
    "                loss_y = np.array([lim_cuts.t.loc[loss1_i,'Ploss'], lim_cuts.t.loc[loss2_i,'Ploss']])\n",
    "                contam_line = np.polyfit(x,contam_y,1)\n",
    "                loss_line = np.polyfit(x,loss_y,1)\n",
    "                loss_lim_cut = (loss_lim-loss_line[1])/loss_line[0]\n",
    "\n",
    "    sortby_contam = lim_cuts.t.iloc[(lim_cuts.t['Pcontamination']).argsort()].reset_index()\n",
    "    min_contam = sortby_contam.loc[0,'Pcontamination']\n",
    "    max_contam = sortby_contam.loc[len(sortby_contam)-1,'Pcontamination']\n",
    "    # if all contam below lim, contam_lim_cut is max cut\n",
    "    if min_contam < contam_lim and max_contam < contam_lim:\n",
    "        contam_case = 'below lim'\n",
    "        contam_lim_cut = lim_cuts.t.loc[len(lim_cuts.t)-1,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all contam above lim, contam_lim_cut is max cut with min% contam\n",
    "        if min_contam > contam_lim and max_contam > contam_lim:\n",
    "            contam_case = 'above lim'\n",
    "            a = np.where(lim_cuts.t['Pcontamination'] == min_contam)[0]\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            contam_lim_cut = c.loc[len(c)-1,'PSF Chi-Square Cut']\n",
    "        # else if contam crosses lim at some point, contam_lim_cut is max cut with max% contam <= contam_lim\n",
    "        else:\n",
    "            contam_case = 'crosses lim'\n",
    "            valid_cuts = sortby_contam[sortby_contam['Pcontamination'] <= contam_lim]\n",
    "            a = np.where(lim_cuts.t['Pcontamination'] == valid_cuts.loc[len(valid_cuts)-1,'Pcontamination'])[0]\n",
    "            # sort by cuts\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            # get midpoint of contam1 and contam2 (two points on either side of lim)\n",
    "            contam1_i = np.where(lim_cuts.t['PSF Chi-Square Cut'] == c.loc[len(c)-1,'PSF Chi-Square Cut'])[0][0]\n",
    "            if lim_cuts.t.loc[contam1_i,'Pcontamination'] == contam_lim:\n",
    "                contam_lim_cut = lim_cuts.t.loc[contam1_i,'PSF Chi-Square Cut']\n",
    "            else:\n",
    "                contam2_i = contam1_i + 1\n",
    "                x = np.array([lim_cuts.t.loc[contam1_i,'PSF Chi-Square Cut'], lim_cuts.t.loc[contam2_i,'PSF Chi-Square Cut']])\n",
    "                contam_y = np.array([lim_cuts.t.loc[contam1_i,'Pcontamination'], lim_cuts.t.loc[contam2_i,'Pcontamination']])\n",
    "                loss_y = np.array([lim_cuts.t.loc[contam1_i,'Ploss'], lim_cuts.t.loc[contam2_i,'Ploss']])\n",
    "                contam_line = np.polyfit(x,contam_y,1)\n",
    "                loss_line = np.polyfit(x,loss_y,1)\n",
    "                contam_lim_cut = (contam_lim-contam_line[1])/contam_line[0]\n",
    "\n",
    "    get_lim_cuts_data(loss_lim_cut, 'loss_lim_cut', baseline_ix=lc_info['baseline_rev_ix'])\n",
    "    get_lim_cuts_data(contam_lim_cut, 'contam_lim_cut', baseline_ix=lc_info['baseline_rev_ix'])\n",
    "\n",
    "    return contam_lim_cut, loss_lim_cut, contam_case, loss_case\n",
    "\n",
    "def get_lim_cuts_table(stn_cut, cut_start, cut_stop, cut_step, baseline_ix=None):\n",
    "    print('abs(uJy/duJy) cut at %0.2f \\nx2 cut from %0.2f to %0.2f inclusive, with step size %d' % (stn_cut,cut_start,cut_stop,cut_step))\n",
    "\n",
    "    if baseline_ix is None: \n",
    "        baseline_ix = lc_info['baseline_rev_ix']\n",
    "\n",
    "    lim_cuts = pdastrostatsclass(columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                          'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Ploss', 'Pcontamination',\n",
    "                                          'Nbad,cut 3<stn<=5', 'Nbad,cut 5<stn<=10', 'Nbad,cut 10<stn', 'Nbad,kept 3<stn<=5', 'Nbad,kept 5<stn<=10', 'Nbad,kept 10<stn'])\n",
    "    \n",
    "    # static cut at x2 = 50\n",
    "    x2cut_50 = np.where(lc_info['lc'].t['chi/N'] < 50)[0]\n",
    "    print('Static chi square cut at 50: %0.2f%% cut for baseline' % (100*len(AnotB(baseline_ix,x2cut_50))/len(baseline_ix)))\n",
    "\n",
    "    # good baseline measurement indices\n",
    "    b_good_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=baseline_ix)\n",
    "    b_bad_i = AnotB(baseline_ix, b_good_i)\n",
    "    # for different x2 cuts decreasing from 50\n",
    "    for cut in range(cut_start,cut_stop+1,cut_step):\n",
    "        # kept baseline measurement indices\n",
    "        b_kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=cut,indices=baseline_ix)\n",
    "        b_cut_i = AnotB(baseline_ix, b_kept_i)\n",
    "\n",
    "        if 100*(len(b_kept_i)/len(baseline_ix)) < 10:\n",
    "            # less than 10% of measurements kept, so no chi-square cuts beyond this point are valid\n",
    "            print(f'# At cut {cut}, less than 10% of measurements are kept ({100*(len(b_kept_i)/len(baseline_ix)):0.2f}% kept)--skipping...')\n",
    "            continue\n",
    "        else: \n",
    "            df = pd.DataFrame([[cut, len(baseline_ix), # N\n",
    "                                len(b_good_i), # Ngood\n",
    "                                len(b_bad_i), # Nbad\n",
    "                                len(b_kept_i), # Nkept\n",
    "                                len(b_cut_i), # Ncut\n",
    "                                len(AandB(b_good_i,b_kept_i)), # Ngood,kept\n",
    "                                len(AandB(b_good_i,b_cut_i)), # Ngood,cut\n",
    "                                len(AandB(b_bad_i,b_kept_i)), # Nbad,kept\n",
    "                                len(AandB(b_bad_i,b_cut_i)), # Nbad,cut\n",
    "                                100*len(AandB(b_good_i,b_kept_i))/len(baseline_ix), # Ngood,kept/Nbaseline\n",
    "                                100*len(AandB(b_good_i,b_cut_i))/len(baseline_ix), # Ngood,cut/Nbaseline \n",
    "                                100*len(AandB(b_bad_i,b_kept_i))/len(baseline_ix), # Nbad,kept/Nbaseline\n",
    "                                100*len(AandB(b_bad_i,b_cut_i))/len(baseline_ix), # Nbad,cut/Nbaseline\n",
    "                                100*len(AandB(b_good_i,b_kept_i))/len(b_good_i), # Ngood,kept/Ngood\n",
    "                                100*len(AandB(b_good_i,b_cut_i))/len(b_good_i), # Ngood,cut/Ngood = Loss\n",
    "                                100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i), # Nbad,kept/Nkept = Contamination\n",
    "                                len(AandB(AandB(b_bad_i,b_cut_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-3,uplim=5,exclude_lowlim=True))), # Nbad,cut 3<stn<=5\n",
    "                                len(AandB(AandB(b_bad_i,b_cut_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=5,uplim=10,exclude_lowlim=True))), # Nbad,cut 5<stn<=10\n",
    "                                len(AandB(AandB(b_bad_i,b_cut_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=10,exclude_lowlim=True))), # Nbad,cut 10<stn \n",
    "                                len(AandB(AandB(b_bad_i,b_kept_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-3,uplim=5,exclude_lowlim=True))), # Nbad,kept 3<stn<=5\n",
    "                                len(AandB(AandB(b_bad_i,b_kept_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=5,uplim=10,exclude_lowlim=True))), # Nbad,kept 5<stn<=10\n",
    "                                len(AandB(AandB(b_bad_i,b_kept_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=10,exclude_lowlim=True))), # Nbad,kept 10<stn \n",
    "                                ]], columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                            'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Ploss', 'Pcontamination',\n",
    "                                            'Nbad,cut 3<stn<=5', 'Nbad,cut 5<stn<=10', 'Nbad,cut 10<stn', 'Nbad,kept 3<stn<=5', 'Nbad,kept 5<stn<=10', 'Nbad,kept 10<stn'])\n",
    "            lim_cuts.t = pd.concat([lim_cuts.t,df],ignore_index=True)\n",
    "    return lim_cuts\n",
    "\n",
    "if lim_to_prioritize != 'loss_lim' and lim_to_prioritize != 'contam_lim':\n",
    "    print(\"ERROR: lim_to_prioritize must be 'loss_lim' or 'contam_lim'!\")\n",
    "    sys.exit()\n",
    "print('Contamination limit: %0.2f%%\\nLoss limit: %0.2f%%' % (contam_lim,loss_lim))\n",
    "\n",
    "lim_cuts = get_lim_cuts_table(stn_cut, cut_start, cut_stop, cut_step, baseline_ix=lc_info['baseline_rev_ix'])\n",
    "if lim_cuts.t.empty:\n",
    "    raise RuntimeError('No cuts kept more than 10%% of measurements--chi-square cut not applicable for this SN!')\n",
    "\n",
    "contam_lim_cut, loss_lim_cut, contam_case, loss_case = get_lim_cuts(lim_cuts)\n",
    "lc_info['contam_case'] = contam_case\n",
    "lc_info['loss_case'] = loss_case\n",
    "lc_info['contam_lim_cut'] = contam_lim_cut\n",
    "lc_info['loss_lim_cut'] = loss_lim_cut\n",
    "\n",
    "print('\\nContamination cut according to given contam_limit, with %0.2f%% contamination and %0.2f%% loss: %0.2f' % (lc_info['contam_lim_cut_Pcontamination'], lc_info['contam_lim_cut_Ploss'], contam_lim_cut))\n",
    "if lc_info['contam_case'] == 'above lim':\n",
    "    print('WARNING: Contamination cut not possible with contamination <= contam_lim %0.1f!' % contam_lim)\n",
    "print('Loss cut according to given loss_limit, with %0.2f%% contamination and %0.2f%% loss: %0.2f' % (lc_info['loss_lim_cut_Pcontamination'], lc_info['loss_lim_cut_Ploss'], loss_lim_cut))\n",
    "if lc_info['loss_case'] == 'above lim':\n",
    "    print('WARNING: Loss cut not possible with loss <= loss_lim %0.2f!' % loss_lim)\n",
    "\n",
    "final_cut = choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case)\n",
    "lc_info['final_cut'] = final_cut\n",
    "        \n",
    "if np.isnan(final_cut):\n",
    "    print('\\nERROR: Final suggested chi-square cut could not be determined. We suggest rethinking your contamination and loss limits.')\n",
    "    lc_info['Pcontamination'] = np.nan\n",
    "    lc_info['Ploss'] = np.nan\n",
    "else:\n",
    "    if final_cut==contam_lim_cut:\n",
    "        lc_info['Pcontamination'] = lc_info['contam_lim_cut_Pcontamination']\n",
    "        lc_info['Ploss'] = lc_info['contam_lim_cut_Ploss']\n",
    "    else:\n",
    "        lc_info['Pcontamination'] = lc_info['loss_lim_cut_Pcontamination']\n",
    "        lc_info['Ploss'] = lc_info['loss_lim_cut_Ploss']\n",
    "    print('\\nFinal suggested chi-square cut is %0.2f, with %0.2f%% contamination and %0.2f%% loss.' % (final_cut, lc_info['Pcontamination'], lc_info['Ploss']))\n",
    "    if (lc_info['Pcontamination'] > contam_lim):\n",
    "        print('WARNING: Final cut\\'s contamination %0.2f%% exceeds contam_lim %0.2f%%!' % (lc_info['Pcontamination'],contam_lim))\n",
    "    if (lc_info['Ploss'] > loss_lim):\n",
    "        print('WARNING: Final cut\\'s loss exceeds %0.2f%% loss_lim %0.2f%%!' % (lc_info['Ploss'],loss_lim))\n",
    "\n",
    "plot_lim_cuts(lim_cuts, contam_lim_cut, loss_lim_cut)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c) Confirm or override the final cut, apply final cut, and optionally plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the light curve before and after the applied chi-square cut?:\n",
    "plot = False\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the chi-square cut plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the final chi-square cut and update 'Mask' column\n",
    "\n",
    "answer = input('Accept final chi-square cut of %0.2f (y/n):' % float(lc_info['final_cut']))\n",
    "if answer != 'y':\n",
    "    final_cut_override = float(input('Overriding final chi-square cut; enter manual cut: '))\n",
    "\n",
    "    b_good_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=lc_info['baseline_i'])\n",
    "    b_bad_i = AnotB(lc_info['baseline_i'], b_good_i)\n",
    "    b_kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=final_cut_override,indices=lc_info['baseline_i'])\n",
    "    b_cut_i = AnotB(lc_info['baseline_i'], b_kept_i)\n",
    "    lc_info['Ploss'] = 100*len(AandB(b_good_i,b_cut_i))/len(b_good_i)\n",
    "    lc_info['Pcontamination'] = 100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i)\n",
    "    lc_info['final_cut'] = final_cut_override\n",
    "\n",
    "    print('Overridden: final cut is now %0.2f, with contamination %0.2f%% and loss %0.2f%%' % (lc_info['final_cut'],lc_info['Pcontamination'],lc_info['Ploss']))\n",
    "\n",
    "print(f'Applying chi-square cut of {lc_info[\"final_cut\"]:0.2f}...')\n",
    "kept_ix = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=lc_info['final_cut'])\n",
    "cut_ix = AnotB(lc_info['lc'].getindices(), kept_ix)\n",
    "update_mask_col(lc_info['lc'].t, flag_chisquare, cut_ix)\n",
    "print('Success')\n",
    "\n",
    "num_cut = len(cut_ix)\n",
    "percent_cut = 100 * num_cut/len(lc_info['lc'].t)\n",
    "print_statistics(num_cut, percent_cut, flag_chisquare, add2title='chi-square cut')\n",
    "f.write(f'\\n\\n## Chi-square cut\\nNumber of cut measurements: {num_cut:d}\\nPercent of total measurements cut: {percent_cut:0.2f}%\\nHex value in \"Mask\" column: 0x1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Control light curves:  3σ-clipped average cut\n",
    "\n",
    "While the chi-square and uncertainty cuts are effective in cutting out a majority of the bad measurements, tricky cases may require a larger set of control light curves that can be used as a basis of comparison for inconsistent flux. In order to account for this inconsistent flux, we can obtain ~8 quality control forced photometry light curves in a 17\" circle pattern around the SN location OR around a nearby bright object that may be poorly subtracting. Then, we use statistics from these control light curves to cut bad measurements from the SN light curve.\n",
    "\n",
    "For a given epoch, we have 1 SN measurement for which we examine 8 control measurements within the same epoch. We know that if the control light curve measurements are NOT consistent with 0, this indicates something wrong with this epoch, so the SN measurement is unreliable. Therefore, we obtain statistics for the control light curves by calculating the 3σ-clipped average of the control flux. \n",
    "\n",
    "For the given epoch, we cut the SN measurement for which the returned control statistics fulfill any of the following criteria: \n",
    "- A returned chi-square > 2.5\n",
    "- A returned abs(flux/dflux) > 3.0\n",
    "- Number of clipped/\"bad\" measurements in the 3σ-clipped average > 2\n",
    "- Number of used/\"good\" measurements in the 3σ-clipped average < 4\n",
    "\n",
    "Measurements not fulfilling any of the criteria above but with Nclip > 0 are flagged as questionable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the bound for an epoch's maximum chi-square \n",
    "# (if x2 > x2_max, flag SN measurement):\n",
    "x2_max = 2.5\n",
    "\n",
    "# Enter the bound for an epoch's maximum abs(flux/dflux) ratio \n",
    "# (if abs(flux/dflux) > stn_max, flag SN measurement):\n",
    "stn_max = 3.0\n",
    "\n",
    "# Enter the bound for an epoch's maximum number of clipped control measurements\n",
    "# (if Nclip > Nclip_max, flag SN measurement):\n",
    "Nclip_max = 2\n",
    "\n",
    "# Enter the bound for an epoch's minimum number of good control measurements\n",
    "# (if Ngood < Ngood_min, flag SN measurement):\n",
    "Ngood_min = 4\n",
    "\n",
    "# Plot the light curve before and after the applied uncertainty cut?:\n",
    "plot = False\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the control light curve cut plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply control light curve cut and save flags in 'Mask' column\n",
    "\n",
    "def get_control_stats(Ncontrols):\n",
    "    print('\\nCalculating control light curve statistics...')\n",
    "\n",
    "    # construct arrays for control lc data\n",
    "    uJy = np.full((Ncontrols, len(lc_info['lc'].t['MJD'])), np.nan)\n",
    "    duJy = np.full((Ncontrols, len(lc_info['lc'].t['MJD'])), np.nan)\n",
    "    Mask = np.full((Ncontrols, len(lc_info['lc'].t['MJD'])), 0, dtype=np.int32)\n",
    "    \n",
    "    for control_index in range(1,Ncontrols+1):\n",
    "        if (len(controls[control_index].t) != len(lc_info['lc'].t['MJD'])) or (np.array_equal(lc_info['lc'].t['MJD'], controls[control_index].t['MJD']) is False):\n",
    "            raise RuntimeError(f'ERROR: SN lc not equal to control lc for control_index {control_index}! Rerun or debug verify_mjds().')\n",
    "        else:\n",
    "            uJy[control_index-1,:] = controls[control_index].t['uJy']\n",
    "            duJy[control_index-1,:] = controls[control_index].t[dflux_colnames[control_index]]\n",
    "            Mask[control_index-1,:] = controls[control_index].t['Mask']\n",
    "\n",
    "    c2_param2columnmapping = lc_info['lc'].intializecols4statparams(prefix='c2_',format4outvals='{:.2f}',skipparams=['converged','i'])\n",
    "\n",
    "    for index in range(uJy.shape[-1]):\n",
    "        pda4MJD = pdastrostatsclass()\n",
    "        pda4MJD.t['uJy'] = uJy[0:,index]\n",
    "        pda4MJD.t[dflux_colnames[0]] = duJy[0:,index]\n",
    "        pda4MJD.t['Mask'] = np.bitwise_and(Mask[0:,index], flag_chisquare|flag_uncertainty)\n",
    "        \n",
    "        pda4MJD.calcaverage_sigmacutloop('uJy',noisecol=dflux_colnames[0],maskcol='Mask',maskval=(flag_chisquare|flag_uncertainty),verbose=1,Nsigma=3.0,median_firstiteration=True)\n",
    "        lc_info['lc'].statresults2table(pda4MJD.statparams, c2_param2columnmapping, destindex=index)\n",
    "\n",
    "def controls_cut():\n",
    "    print('Flagging SN light curve based on control light curve statistics...')\n",
    "\n",
    "    lc_info['lc'].t['c2_abs_stn'] = lc_info['lc'].t['c2_mean']/lc_info['lc'].t['c2_mean_err']\n",
    "\n",
    "    # flag measurements according to given bounds\n",
    "    flag_x2_i = lc_info['lc'].ix_inrange(colnames=['c2_X2norm'], lowlim=x2_max, exclude_lowlim=True)\n",
    "    flag_stn_i = lc_info['lc'].ix_inrange(colnames=['c2_abs_stn'], lowlim=stn_max, exclude_lowlim=True)\n",
    "    flag_nclip_i = lc_info['lc'].ix_inrange(colnames=['c2_Nclip'], lowlim=Nclip_max, exclude_lowlim=True)\n",
    "    flag_ngood_i = lc_info['lc'].ix_inrange(colnames=['c2_Ngood'], uplim=Ngood_min, exclude_uplim=True)\n",
    "    lc_info['lc'].t.loc[flag_x2_i,'Mask'] |= flag_controls_x2\n",
    "    lc_info['lc'].t.loc[flag_stn_i,'Mask'] |= flag_controls_stn\n",
    "    lc_info['lc'].t.loc[flag_nclip_i,'Mask'] |= flag_controls_Nclip\n",
    "    lc_info['lc'].t.loc[flag_ngood_i,'Mask'] |= flag_controls_Ngood\n",
    "\n",
    "    # update mask column with control light curve cut on any measurements flagged according to given bounds\n",
    "    zero_Nclip_i = lc_info['lc'].ix_equal('c2_Nclip', 0)\n",
    "    unmasked_i = lc_info['lc'].ix_unmasked('Mask', maskval=flag_controls_x2|flag_controls_stn|flag_controls_Nclip|flag_controls_Ngood)\n",
    "    lc_info['lc'].t.loc[AnotB(unmasked_i,zero_Nclip_i),'Mask'] |= flag_controls_questionable\n",
    "    lc_info['lc'].t.loc[AnotB(lc_info['lc'].getindices(),unmasked_i),'Mask'] |= flag_controls_bad\n",
    "\n",
    "    # copy over SN's control cut flags to control light curve 'Mask' column\n",
    "    flags_arr = np.full(lc_info['lc'].t['Mask'].shape, (flag_badday|flag_controls_questionable|flag_controls_x2|flag_controls_stn|flag_controls_Nclip|flag_controls_Ngood))\n",
    "    flags_to_copy = np.bitwise_and(lc_info['lc'].t['Mask'], flags_arr)\n",
    "    for control_index in range(1,Ncontrols+1):\n",
    "        controls[control_index].t['Mask'] = controls[control_index].t['Mask'].astype(np.int32)\n",
    "        if len(controls[control_index].t) < 1:\n",
    "            continue\n",
    "        elif len(controls[control_index].t) == 1:\n",
    "            controls[control_index].t.loc[0,'Mask']= int(controls[control_index].t.loc[0,'Mask']) | flags_to_copy\n",
    "        else:\n",
    "            controls[control_index].t['Mask'] = np.bitwise_or(controls[control_index].t['Mask'], flags_to_copy)\n",
    "\n",
    "def print_flag_stats():\n",
    "    percent_cut = 100 * len(lc_info['lc'].ix_masked('Mask', maskval=flag_controls_bad)) / len(lc_info['lc'].t) \n",
    "    percent_questionable = 100 * len(lc_info['lc'].ix_masked('Mask', maskval=flag_controls_questionable)) / len(lc_info['lc'].t)\n",
    "\n",
    "    x2_max_pcnt = 100 * len(lc_info['lc'].ix_masked('Mask', maskval=flag_controls_x2)) / len(lc_info['lc'].t)\n",
    "    stn_max_pcnt = 100 * len(lc_info['lc'].ix_masked('Mask', maskval=flag_controls_stn)) / len(lc_info['lc'].t)\n",
    "    Nclip_max_pcnt = 100 * len(lc_info['lc'].ix_masked('Mask', maskval=flag_controls_Nclip)) / len(lc_info['lc'].t)\n",
    "    Ngood_min_pcnt = 100 * len(lc_info['lc'].ix_masked('Mask', maskval=flag_controls_Ngood)) / len(lc_info['lc'].t)\n",
    "\n",
    "    print('\\nLength of SN light curve: %d' % len(lc_info['lc'].t))\n",
    "    print('Percent of data above x2_max bound: %0.2f%%' % x2_max_pcnt)\n",
    "    print('Percent of data above stn_max bound: %0.2f%%' % stn_max_pcnt)\n",
    "    print('Percent of data above Nclip_max bound: %0.2f%%' % Nclip_max_pcnt)\n",
    "    print('Percent of data below Ngood_min bound: %0.2f%%' % Ngood_min_pcnt)\n",
    "    print('Total percent of data flagged as bad: %0.2f%%' % percent_cut)\n",
    "    print('Total percent of data flagged as questionable: %0.2f%%' % percent_questionable)\n",
    "\n",
    "    f.write(f'\\n\\n## Control light curve cut\\nPercent of data above x2_max bound: {x2_max_pcnt:0.2f}%\\nPercent of data above stn_max bound: {stn_max_pcnt:0.2f}%\\nPercent of data above Nclip_max bound: {Nclip_max_pcnt:0.2f}%\\nPercent of data below Ngood_min bound: {Ngood_min_pcnt:0.2f}%')\n",
    "    f.write(f'\\nTotal percent of data flagged as bad: {percent_cut:0.2f}%\\nTotal percent of data flagged as questionable: {percent_questionable:0.2f}%\\nHex value in \"Mask\" column (flagged as \"bad\"): 0x400000\\nHex value in \"Mask\" column (flagged as \"questionable\"): 0x80000')\n",
    "\n",
    "    if plot or percent_cut > 10:\n",
    "        limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "        plot_cut_lc('lc', flag_controls_bad, dflux_colnames[0], add2title='control light curve cut', limits=limits)\n",
    "\n",
    "if load_controls:    \n",
    "    get_control_stats(Ncontrols)\n",
    "    controls_cut()\n",
    "    print_flag_stats()\n",
    "else:\n",
    "    print('Load_controls set to False! Skipping...')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ATLAS light curve with a combination of previous cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the flags you would like to use to plot kept vs. clean measurements\n",
    "#   - uncertainty cut flag: flag_uncertainty\n",
    "#   - chi-square cut flag: flag_chisquare\n",
    "#   - control light curve cut flag: flag_controls_bad\n",
    "flags = flag_uncertainty | flag_chisquare | flag_controls_bad\n",
    "\n",
    "# Optionally, manually enter the plot's x and y limits:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the light curve\n",
    "\n",
    "limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "plot_cut_lc('lc', flags, dflux_colnames[0], add2title='\\nuncertainty, chi-square, and control light curve cuts', limits=limits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Averaging and cutting bad bins\n",
    "\n",
    "Our goal is to identify and cut out bad MJD bins by taking a 3σ-clipped average of each bin. For each bin, we calculate the 3σ-clipped average of any SN measurements falling within that bin and use that average as our flux for that bin. Because the ATLAS survey takes about 4 exposures every 2 days, we usually average together approximately 4 measurements per epoch. However, out of these 4 exposures, only measurements not cut in the previous methods are averaged in the 3σ-clipped average cut. (The exception to this statement would be the case that all 4 measurements are cut in previous methods; in this case, they are averaged anyway and flagged as a bad bin.)\n",
    "\n",
    "Then we cut any measurements in the SN light curve for the given epoch for which statistics fulfill any of the following criteria: \n",
    "- A returned chi-square > 4.0\n",
    "- Number of measurements averaged < 2\n",
    "- Number of measurements clipped > 1\n",
    "\n",
    "For this part of the cleaning, we still need to improve the cutting at the peak of the SN (important epochs are sometimes cut, maybe due to fast rise, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the MJD bin size in days:\n",
    "mjdbinsize = 1\n",
    "\n",
    "# Should MJD bins with no measurements be translated as NaN (True) \n",
    "# or removed from the averaged light curve (False)?\n",
    "keep_empty_bins = False\n",
    "\n",
    "# After flux is averaged, average magnitudes are calculated using a flux-to-magnitude conversion.\n",
    "# Magnitudes are limits if the dmagnitude is NaN. Enter these magnitudes' sigma limit:\n",
    "flux2mag_sigma_limit = 3\n",
    "\n",
    "# Enter the bound for a bin's maximum number of clipped measurements\n",
    "# (if Nclip > Nclip_max, flag day):\n",
    "Nclip_max = 1\n",
    "\n",
    "# Enter the bound for a bin's minimum number of good measurements\n",
    "# (if Ngood < Ngood_min, flag day):\n",
    "Ngood_min = 2\n",
    "\n",
    "# Enter the bound for a bin's maximum chi-square (if x2 > x2_max, flag day):\n",
    "x2_max = 4.0\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the light curve\n",
    "\n",
    "def average_lc(Nclip_max, Ngood_min, x2_max, flag_badday, flag_ixclip, flag_smallnum, mjdbinsize=1, flux2mag_sigma_limit=3.0, keep_empty_bins=True):\n",
    "    mjd = int(np.amin(lc_info['lc'].t['MJD']))\n",
    "    mjd_max = int(np.amax(lc_info['lc'].t['MJD']))+1\n",
    "\n",
    "    good_i = lc_info['lc'].ix_unmasked('Mask', maskval=flag_chisquare|flag_uncertainty)\n",
    "\n",
    "    while mjd <= mjd_max:\n",
    "        range_i = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=mjd, uplim=mjd+mjdbinsize, exclude_uplim=True)\n",
    "        range_good_i = AandB(range_i,good_i)\n",
    "\n",
    "        # add new row to avglc if keep_empty_bins or any measurements present\n",
    "        if keep_empty_bins or len(range_i) >= 1:\n",
    "            new_row = {'MJDbin':mjd+0.5*mjdbinsize, 'Nclip':0, 'Ngood':0, 'Nexcluded':len(range_i)-len(range_good_i), 'Mask':0}\n",
    "            avglc_index = lc_info['avglc'].newrow(new_row)\n",
    "        \n",
    "        # if no measurements present, flag or skip over day\n",
    "        if len(range_i) < 1:\n",
    "            if keep_empty_bins:\n",
    "                update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "            mjd += mjdbinsize\n",
    "            continue\n",
    "        \n",
    "        # if no good measurements, average values anyway and flag\n",
    "        if len(range_good_i) < 1:\n",
    "            # average flux\n",
    "            lc_info['lc'].calcaverage_sigmacutloop('uJy', noisecol=dflux_colnames[0], indices=range_i, Nsigma=3.0, median_firstiteration=True)\n",
    "            fluxstatparams = deepcopy(lc_info['lc'].statparams)\n",
    "\n",
    "            # average mjd\n",
    "            # SHOULD NOISECOL HERE BE DUJY OR NONE??\n",
    "            lc_info['lc'].calcaverage_sigmacutloop('MJD', noisecol=dflux_colnames[0], indices=fluxstatparams['ix_good'], Nsigma=0, median_firstiteration=False)\n",
    "            avg_mjd = lc_info['lc'].statparams['mean']\n",
    "\n",
    "            # add row and flag\n",
    "            lc_info['avglc'].add2row(avglc_index, {'MJD':avg_mjd, \n",
    "                                                   'uJy':fluxstatparams['mean'], \n",
    "                                                   'duJy':fluxstatparams['mean_err'], \n",
    "                                                   'stdev':fluxstatparams['stdev'],\n",
    "                                                   'x2':fluxstatparams['X2norm'],\n",
    "                                                   'Nclip':fluxstatparams['Nclip'],\n",
    "                                                   'Ngood':fluxstatparams['Ngood'],\n",
    "                                                   'Mask':0})\n",
    "            update_mask_col(lc_info['lc'].t, flag_badday, range_i)\n",
    "            update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "\n",
    "            mjd += mjdbinsize\n",
    "            continue\n",
    "        \n",
    "        # average good measurements\n",
    "        lc_info['lc'].calcaverage_sigmacutloop('uJy', noisecol=dflux_colnames[0], indices=range_good_i, Nsigma=3.0, median_firstiteration=True)\n",
    "        fluxstatparams = deepcopy(lc_info['lc'].statparams)\n",
    "\n",
    "        if fluxstatparams['mean'] is None or len(fluxstatparams['ix_good']) < 1:\n",
    "            update_mask_col(lc_info['lc'].t, flag_badday, range_i)\n",
    "            update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "            mjd += mjdbinsize\n",
    "            continue\n",
    "\n",
    "        # average mjd\n",
    "        # SHOULD NOISECOL HERE BE DUJY OR NONE??\n",
    "        lc_info['lc'].calcaverage_sigmacutloop('MJD', noisecol=dflux_colnames[0], indices=fluxstatparams['ix_good'], Nsigma=0, median_firstiteration=False)\n",
    "        avg_mjd = lc_info['lc'].statparams['mean']\n",
    "\n",
    "        # add row\n",
    "        lc_info['avglc'].add2row(avglc_index, {'MJD':avg_mjd, \n",
    "                                                   'uJy':fluxstatparams['mean'], \n",
    "                                                   'duJy':fluxstatparams['mean_err'], \n",
    "                                                   'stdev':fluxstatparams['stdev'],\n",
    "                                                   'x2':fluxstatparams['X2norm'],\n",
    "                                                   'Nclip':fluxstatparams['Nclip'],\n",
    "                                                   'Ngood':fluxstatparams['Ngood'],\n",
    "                                                   'Mask':0})\n",
    "        \n",
    "        # flag clipped measurements in lc\n",
    "        if len(fluxstatparams['ix_clip']) > 0:\n",
    "            update_mask_col(lc_info['lc'].t, flag_ixclip, fluxstatparams['ix_clip'])\n",
    "        \n",
    "        # if small number within this bin, flag measurements\n",
    "        if len(range_good_i) < 3:\n",
    "            update_mask_col(lc_info['lc'].t, flag_smallnum, range_good_i) # CHANGE TO RANGE_I??\n",
    "            update_mask_col(lc_info['avglc'].t, flag_smallnum, [avglc_index])\n",
    "        # else check sigmacut bounds and flag\n",
    "        else:\n",
    "            is_bad = False\n",
    "            if fluxstatparams['Ngood'] < Ngood_min:\n",
    "                is_bad = True\n",
    "            if fluxstatparams['Nclip'] > Nclip_max:\n",
    "                is_bad = True\n",
    "            if not(fluxstatparams['X2norm'] is None) and fluxstatparams['X2norm'] > x2_max:\n",
    "                is_bad = True\n",
    "            if is_bad:\n",
    "                update_mask_col(lc_info['lc'].t, flag_badday, range_i)\n",
    "                update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "\n",
    "        mjd += mjdbinsize\n",
    "\n",
    "    # convert flux to magnitude and dflux to dmagnitude\n",
    "    for col in ['uJy','duJy']: \n",
    "        lc_info['avglc'].t[col] = lc_info['avglc'].t[col].astype(float)\n",
    "    lc_info['avglc'].flux2mag('uJy','duJy','m','dm', zpt=23.9, upperlim_Nsigma=flux2mag_sigma_limit)\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "    drop_extra_columns('avglc')\n",
    "\n",
    "    for col in ['Nclip','Ngood','Nexcluded','Mask']: \n",
    "        lc_info['avglc'].t[col] = lc_info['avglc'].t[col].astype(np.int32)\n",
    "\n",
    "if len(lc_info['lc'].t) < 1:\n",
    "    print('ERROR: No data in lc so cannot average; exiting... ')\n",
    "    sys.exit()\n",
    "\n",
    "print('Averaging light curve with the following criteria: Nclip_max = %d, Ngood_min = %d, x2_max = %0.2f... ' % (Nclip_max, Ngood_min, x2_max))\n",
    "lc_info['avglc'] = pdastrostatsclass(columns=['MJD','MJDbin','uJy','duJy','stdev','x2','Nclip','Ngood','Nexcluded','Mask'],hexcols=['Mask'])\n",
    "average_lc(Nclip_max, Ngood_min, x2_max, flag_badday, flag_ixclip, flag_smallnum, keep_empty_bins=keep_empty_bins)\n",
    "\n",
    "# print statistics and plot\n",
    "num_cut = len(lc_info['avglc'].ix_masked('Mask', flag_badday))\n",
    "percent_cut = (num_cut/len(lc_info['avglc'].t)) * 100\n",
    "print(f'\\nNumber of cut measurements: {num_cut:d}\\nPercent of total measurements cut: {percent_cut:0.2f}%')\n",
    "f.write(f'\\n\\n## Averaging and cutting bad bins\\nNumber of cut measurements: {num_cut}\\nPercent of total measurements cut: {percent_cut:0.2f}%\\nHex value in \"Mask\" column: 0x800000')\n",
    "if percent_cut > 10:\n",
    "    print(f'WARNING: percent of total measurements cut is greater than 10%')\n",
    "limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "plot_cut_lc('avglc', flag_chisquare|flag_uncertainty|flag_badday, 'duJy', limits=limits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: save the SN light curve with the new `'Mask'` and `'duJy_new'` columns\n",
    "\n",
    "Hex values in the `'Mask'` column for each cut's flag:\n",
    "- Uncertainty cut: 0x2\n",
    "- Chi-square cut: 0x1\n",
    "- Control light curve cut: 0x400000\n",
    "- Bad day (for averaged light curves): 0x800000\n",
    "\n",
    "You can combine these hex values together to create certain combinations of cuts that define a \"bad\" measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the light curve with the new 'Mask' column?\n",
    "save = False\n",
    "\n",
    "# Overwrite the old light curve file?\n",
    "overwrite_old_lc = False\n",
    "\n",
    "# If not overwriting old light curve file, enter new filename:\n",
    "filename_new = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save light curve\n",
    "\n",
    "if save:\n",
    "    print('Saving light curve with updated mask column...')\n",
    "    if overwrite_old_lc:\n",
    "        print('Overwriting old light curve file at %s... ' % filename)\n",
    "        save_lc('lc',filename,overwrite=True)\n",
    "    else:\n",
    "        print('Writing new file at %s... ' % filename_new)\n",
    "        save_lc('lc',filename_new,overwrite=True)\n",
    "else:\n",
    "    print('Did not save SN light curve')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary of all cuts and corrections\n",
    "\n",
    "f.close()\n",
    "f1 = open(f'{lc_info[\"tnsname\"]}_output.md')\n",
    "content = f1.read()\n",
    "print()\n",
    "print(content)\n",
    "f1.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
